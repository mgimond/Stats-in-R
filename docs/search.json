[
  {
    "objectID": "ANOVA.html",
    "href": "ANOVA.html",
    "title": "ANOVA",
    "section": "",
    "text": "Last modified on 2023-04-06\nPackage used in this tutorial:"
  },
  {
    "objectID": "ANOVA.html#the-variance-ratio-method",
    "href": "ANOVA.html#the-variance-ratio-method",
    "title": "ANOVA",
    "section": "2.1 The variance-ratio method",
    "text": "2.1 The variance-ratio method\nAn ANOVA test seeks to compare the spread between the batches (technically referred to as levels).\nThe first step is to sum the square of the distances between each value (from all levels) to the grand mean computed from all values (plotted as a dark dashed line in the following graphic). We’ll call this value the total sum of squares for the mean (\\(SSE_{mean}\\)). It’s calculated as follows:\n\\[\nSSE_{mean} = \\sum (y - \\bar{\\bar y})^2\n\\]\n\n\n\nwhere \\(\\bar{\\bar{y}}\\) is the mean for all values. In this example, \\(\\bar{\\bar y}\\) equals 24.7. In the following plot, we spread out the values in each level (for clarity) and measure their distances to the grand mean. Each level is assigned a unique color to distinguish the different batches.\n\n\n\n\n\nNext, we compare the values in each level to their respective level means. Similar to the way we compute the \\(SSE_{mean}\\), we sum the squared differences for each level as follows:\n\\[\nSSE = \\sum (y_{batch1} - \\bar y_{batch1})^2 + \\sum (y_{batch2} - \\bar y_{batch2})^2 + \\sum (y_{batch3} - \\bar y_{batch3})^2\n\\]\nWhere \\(SSE\\) is the error sum of squares.\n\n\n\n\n\nIf the mean values of all three levels are the same, their horizontal lines should line up with the grand mean and both \\(SSE_{mean}\\) and \\(SSE\\) should be equal, if not, \\(SSE\\) will be less than \\(SSE_{mean}\\) (the distance between the points and their respective level mean will always be equal or shorter than their distances to the overall mean). The difference between \\(SSE_{mean}\\) and \\(SSE\\) is called the treatment sum of squares (also referred to as the model sum of squares):\n\\[\nSSR = SSE_{mean} - SSE\n\\]\nIf SSR is close to 0, then the differences between the levels is small, if SSR is large, then two or more of the levels are significantly different from one another. SSR is, in fact, the squared difference between each group’s mean and the grand mean. Think of two competing models to predict the observed values: the grand mean, and each level’s mean. If the level means are close to the grand mean, then both the level means and grand mean will generate the same expected values. If they are different, then they will generate different expected values. So the greater the SSR, the more different the predicted values (and therefore the more different the means).\n\n\n\n\n\nThe goal is to compare the variability in \\(SSE\\) to that in \\(SSR\\), however, \\(SSR\\) can take on much larger values than \\(SSA\\) (because \\(SSR\\) is measuring from the group means and not the individual values). To remedy this bias, we compute the mean squares from both values by dividing the sum of squares by the degrees of freedom. For \\(SSR\\), this gives us: \\[\nMSR = \\frac{SSR}{p-1}\n\\]\nwhere \\(MSR\\) is the mean square for treatments (or mean square for model), and \\(p\\) is the number of levels (3 in this example).\nLikewise, we can compute the mean square for error as:\n\\[\nMSE = \\frac{SSE}{n-p}\n\\]\nwhere \\(n\\) is the total number of observations.\nNext, we compute the \\(F\\)-statistic (or \\(F\\)-ratio) as:\n\\[\nF = \\frac{MSR}{MSE}\n\\]\n\\(F\\) gives us the proportion of the overall spread in the measurements that can be explained by the levels vs. the proportion of the overall spread in the measurements not explained by the levels. A value of \\(F\\) that approaches \\(1\\) indicates that little to none of the variability in the measurements can be explained by the levels suggesting that differences in their mean may be due to random noise only. If \\(F\\) is much larger than one, then the spreads between levels are quite different (meaning that these differences are large contributors to the overall spread) suggesting that the observed differences in mean values are significant too.\nThe following graphic shows the spread for all values in the data (right-side plot) and its spread broken down by levels (left-side plot). In this case, it seems that a good chunk of the overall spread can be explained by the differences in spread between levels. If this is still unclear, picture the plots with the measurements for batch 2 removed; what you would note is that the overall spread will be noticeably reduced suggesting that the measurements in level two alone were an important contributor to the overall spread.\n\n\n\n\n\nContrast this last example with the following dataset where the differences in spread (and mean) between levels are negligible. Removing anyone of the batches from the dataset would have a negligible impact on the overall spread in the measurements. We would say that little of the spread (variability) in the overall measurements can be explained by differences in measurements between levels; this would result in an \\(F\\)-ratio close to \\(1\\).\n\n\n\n\n\nTo assess whether a value of \\(F\\) is significantly greater than 1, we must compare our observed \\(F\\) to a distribution of \\(F\\)’s we would expect to get if the means between levels were not different (we refer to the hypothesized \\(F\\) values as \\(F_{Ho}\\)).\nIn the following plot, a hypothetical \\(F\\) ratio (plotted as a vertical red line) is located to the right of the distribution of \\(F_{Ho}\\) values (delineated in a black line in the following plot). The shaded pink area to the right of the hypothetical \\(F\\) represents the fraction of \\(F_{Ho}\\) that would be more extreme than our observed \\(F\\). The goal is to assess whether or not we feel comfortable in concluding that our observed \\(F\\) is significantly different from an \\(F\\) we would expect to get if the means between batches were all the same."
  },
  {
    "objectID": "ANOVA.html#an-example-in-r",
    "href": "ANOVA.html#an-example-in-r",
    "title": "ANOVA",
    "section": "2.2 An example in R",
    "text": "2.2 An example in R\nIn this working example, we compare fecal coliform counts (represented as the log10 of organisms per 100 ml) in the Illinois River between seasons (summer, fall, winter and spring) across six years (data from Millard and Neerchal, 2001).\n\n\n\nYear\nSummer\nFall\nWinter\nSpring\n\n\n\n\n1971\n2.00\n1.45\n1.45\n1.34\n\n\n1972\n2.34\n2.08\n1.76\n1.72\n\n\n1973\n2.48\n2.32\n2.08\n2.04\n\n\n1974\n2.63\n2.45\n2.36\n2.15\n\n\n1975\n2.81\n2.70\n2.49\n2.51\n\n\n1976\n3.20\n3.04\n2.70\n3.11\n\n\n\nLet’s first generate the data. We will create two data frames of the same data. One will be in a wide format, the other in a long format. We will use the former to generate a plot and the latter will be used in the ANOVA analysis.\n\n# Create a data frame\ndat <- data.frame( Year = c(1971,1972,1973,1974,1975,1976),\n                   Summer = c(2.00, 2.34, 2.48, 2.63, 2.81, 3.20),\n                   Fall = c(1.45, 2.08, 2.32, 2.45, 2.70, 3.04),\n                   Winter = c(1.45, 1.76, 2.08, 2.36, 2.49, 2.70),\n                   Spring = c(1.34, 1.72, 2.04, 2.15, 2.51, 3.11))\n\n# However, to run an ANOVA, the data needs to be in a long form\nlibrary(tidyr)\ndat.long <- gather(dat, key = \"Season\", value=\"Value\", -Year)\n\nNext, let’s plot the points then add the grand mean (black line) and each season’s mean (red lines) to the plot.\n\n# Create an empty plot\nplot( NULL, xlim= c(0.5,4.5), ylim=c(1, 3.5), axes=FALSE, xlab=NA,ylab=\"Fecal Coliform\")\nbox()\naxis(1, labels = names(dat)[-1], at=c(1,2,3,4))\naxis(2, las=2, )\npoints(rep(1,nrow(dat)), dat$Summer, pch=1)\npoints(rep(2,nrow(dat)), dat$Fall, pch=2)\npoints(rep(3,nrow(dat)), dat$Winter, pch=3)\npoints(rep(4,nrow(dat)), dat$Spring, pch=4)\n\n# Add the grand mean\nabline(h = mean(dat.long$Value), lty=3)\n\n# Add each group's mean\nlines(c(.8,1.2), rep(mean(dat$Summer),2), col=\"red\" )\nlines(c(1.8,2.2), rep(mean(dat$Fall),2), col=\"red\" )\nlines(c(2.8,3.2), rep(mean(dat$Winter),2), col=\"red\" )\nlines(c(3.8,4.2), rep(mean(dat$Spring),2), col=\"red\" )\n\n\n\n\n\n\nA quick glance at the plot suggests that the variance within each season is large relative to the differences in means between each seasons. But could summer values be slightly higher than those of other seasons? We check this assumption with an ANOVA test.\n\n2.2.1 Computing ANOVA the hard way\n\nSSE.m <- sum( (dat.long$Value - mean(dat.long$Value))^2 ) \nSSE <- sum( (dat$Summer - mean(dat$Summer))^2 +\n            (dat$Fall - mean(dat$Fall))^2 +\n            (dat$Winter - mean(dat$Winter))^2 + \n            (dat$Spring - mean(dat$Spring))^2 )  \nSSR <- SSE.m - SSE\nMSR <- SSR / 3   # (p - 1) or 3 degrees of freedom\nMSE <- SSE / 20  # (n - p) or 20 degrees of freedom\nFratio <- MSR/MSE\np.val  <- pf(Fratio, 3, 20,lower.tail=FALSE)\n\nThe function pf() compares our \\(F\\)-ratio to the distribution of \\(F_{Ho}\\) values one would expect to get if the means between seasons were not different. The two numbers following the \\(F\\)-ratio value are the degrees of freedom for the \\(MSR\\) and the \\(MSE\\) calculated from \\((p-1)\\) and \\((n-p)\\) respectively.\n\n\n\n\n\nThe computed statistics are:\n\n\n\nStatistic\nValue\n\n\n\n\nSSE(mean)\n6.11\n\n\nSSE\n5.35\n\n\nSSR\n0.77\n\n\nMSR\n0.26\n\n\nMSE\n0.27\n\n\nF\n0.96\n\n\np\n0.433\n\n\n\nOur calculated p-value is 0.433 indicating that about 43% of the \\(F_{Ho}\\) values are more extreme than ours. So it would be unwise to dismiss the chance that the means between all four seasons are equal to one another.\n\n\n2.2.2 Computing ANOVA the easy way\nWe can use the anova function to compute the \\(F\\)-ratio and the \\(p\\)-value. The function takes as argument a model (a linear regression model in this case) where the dependent variable \\(y\\) is the measurement value and the independent variable \\(x\\) is the level (or seasons in our example). This implementation of ANOVA requires that the season values be in one column (the \\(x\\) column) and that the measurements be in another column (the \\(y\\) column). This requires that we use the long version of our table, dat.long, where the \\(x\\) column is labeled Season and the \\(y\\) column is labeled Value. The first few rows of dat.long look like this:\n\nhead(dat.long)\n\n  Year Season Value\n1 1971 Summer  2.00\n2 1972 Summer  2.34\n3 1973 Summer  2.48\n4 1974 Summer  2.63\n5 1975 Summer  2.81\n6 1976 Summer  3.20\n\n\nThe ANOVA analysis is thus computed as:\n\nanova(lm(Value ~ Season, dat.long))\n\nAnalysis of Variance Table\n\nResponse: Value\n          Df Sum Sq Mean Sq F value Pr(>F)\nSeason     3 0.7666 0.25554  0.9556 0.4328\nResiduals 20 5.3483 0.26741               \n\n\nThe column Mean Sq displays the mean sum-of-squares for treatment \\(SSR\\), and the error sum-of-square, \\(SSE\\). The \\(F\\)-ratio and \\(p\\)-value are the same as those computed in the last subsection. Again, there is no evidence that the seasons have an influence on the mean concentrations of fecal coliform counts.\n\n\n2.2.3 ANOVA as a regression\nYou’ll note that this approach in computing the ANOVA makes use of the linear regression function lm. This is because a one-way ANOVA is nothing more than a regression between all values in the batches and their levels expressed as categorical values where the number of categorical values is the number of levels minus \\(1\\). In essence the ANOVA is generating the following model:\n\\(Coliform\\ count = a + b(FALL) + c(Winter) + d(Spring)\\)\nSo, if a value belongs to the FALL batch, the model looks like this:\n\\(Coliform\\ count = a + b(1) + c(0) + d(0)\\)\nIf the value belongs to the SUMMER batch, the model looks like this:\n\\(Coliform\\ count = a + b(0) + c(0) + d(0)\\)\nwhere \\(a\\) is the mean value for summer coliform measurements. It follows that the coefficients \\(b\\), \\(c\\) and \\(d\\) are differences in mean values between summer measurements and fall, winter and spring measurements respectively:\n\\(a\\) = mean(dat$Summer) = 2.58\n\\(b\\) = mean(dat$Fall) - mean(dat$Summer) = -0.24\n\\(c\\) = mean(dat$Winter) - mean(dat$Summer) = -0.44\n\\(d\\) = mean(dat$Spring) - mean(dat$Summer) = -0.43\nThese coefficients can be extracted from the lm model via the coefficients function.\n\ncoefficients( lm( Value ~ Season, dat.long) )\n\n (Intercept) SeasonSpring SeasonSummer SeasonWinter \n   2.3400000   -0.1950000    0.2366667   -0.2000000 \n\n\n\nNote that R has a built-in function called aov() that can be used in lieu of lm(). The computed model will be the same but the functions differ slightly in their methods. Examples of its use are presented later in the tutorial."
  },
  {
    "objectID": "ANOVA.html#bonferroni",
    "href": "ANOVA.html#bonferroni",
    "title": "ANOVA",
    "section": "4.1 Bonferroni",
    "text": "4.1 Bonferroni\nThe Bonferroni method can be implemented using the pairwise.t.test function with the first two parameters pointing to the column of values and the column of variables respectively, and the third parameter pointing to the pairwise method.\n\npairwise.t.test( dat.long$Value, dat.long$Season, p.adjust.method = \"bonferroni\")\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  dat.long$Value and dat.long$Season \n\n       Fall Spring Summer\nSpring 1.00 -      -     \nSummer 1.00 0.98   -     \nWinter 1.00 1.00   0.95  \n\nP value adjustment method: bonferroni \n\n\nThe output is a matrix of \\(p\\)-values for different pairs of levels (Fall-Summer, Winter-Summer, etc…). A low \\(P\\)-value indicates significant differences between levels. In our working example, all \\(P\\)-values are relatively high indicating that the means between levels are not statistically significant."
  },
  {
    "objectID": "ANOVA.html#tukeys-hsd",
    "href": "ANOVA.html#tukeys-hsd",
    "title": "ANOVA",
    "section": "4.2 Tukey’s HSD",
    "text": "4.2 Tukey’s HSD\nThe Tukey method is implemented using the TukeyHSD method. The first parameter is an aov object (aov is nearly identical to the anova(lm(...)) method used thus far), and the second variable is the levels column from the data.\n\nTukeyHSD( aov( Value ~ Season, dat.long), \"Season\")\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Value ~ Season, data = dat.long)\n\n$Season\n                    diff        lwr       upr     p adj\nSpring-Fall   -0.1950000 -1.0306503 0.6406503 0.9132083\nSummer-Fall    0.2366667 -0.5989836 1.0723170 0.8569058\nWinter-Fall   -0.2000000 -1.0356503 0.6356503 0.9071983\nSummer-Spring  0.4316667 -0.4039836 1.2673170 0.4870045\nWinter-Spring -0.0050000 -0.8406503 0.8306503 0.9999983\nWinter-Summer -0.4366667 -1.2723170 0.3989836 0.4773334\n\n\nThe Tukey method generates a table of \\(P\\)-values (as opposed to a matrix) for all pairs of levels. Note the slightly lower \\(P\\)-values than those generated with the Bonferroni method. The Tukey HSD method tends to be more conservative than the Bonferri method and is thus more likely to reject the null hypothesis (that the means are equal) than the Bonferri method. This comes with an advantage, however, in that the Tukey test tends to have greater statistical power when there are many different levels (or batches) in our dataset.\nThe Tukey test also generates the 95% confidence intervals (lwr = lower bound and upr = upper bound). If the lower bound is less than \\(0\\) and the upper bound is greater than \\(0\\) we cannot say that the means between both levels are not significantly different at a confidence level of 0.05."
  },
  {
    "objectID": "ANOVA.html#assumptions-of-the-post-hoc-test",
    "href": "ANOVA.html#assumptions-of-the-post-hoc-test",
    "title": "ANOVA",
    "section": "4.3 Assumptions of the post hoc test",
    "text": "4.3 Assumptions of the post hoc test\nThe post hoc procedure assumes that we are only interested in knowing which effect (if any) is different from the grand mean. It makes no assumption about the direction of this difference (i.e. if the level mean is greater than or less than the overall mean). This is analogous to implementing a two-tailed test.\nIf knowing whether the effect mean is greater than or less than the overall mean, a planned contrast procedure (aka planned comparison procedure) should be adopted instead. This procedure is not covered in this tutorial, but can be found in most introductory stats books."
  },
  {
    "objectID": "ANOVA.html#checking-for-interaction-non-additive-model",
    "href": "ANOVA.html#checking-for-interaction-non-additive-model",
    "title": "ANOVA",
    "section": "6.1 Checking for interaction (non-additive model)",
    "text": "6.1 Checking for interaction (non-additive model)\n\nNote that it makes no sense to test for interactivity if you do not have replicates (i.e. if you have just one case per factor level combinations) since you may not be able to distinguish the error terms from the residuals. In the examples that follow, we’ll make use of the datr.long dataset which has (simulated) replicates.\n\nWe need to check that the additive model we’ve chosen is appropriate for this dataset. A graphical approach involves plotting the values as a function of one factor using a line graph (where each line represents the levels of the other factor). This is sometimes referred to as an interactive plot. We’ll make use of the built-in interaction.plot function where one of the factors (Year) will be mapped to the x-axis and the other factor (Season) will be mapped to the line plots. This function does not have a data=... parameter telling it which data frame to work off of so we’ll wrap the function with the with() function which defines the data frame from which the variables are to be read from.\n\nwith(datr.long, interaction.plot(as.factor(Year), Season, Value))\n\n\n\n\nWe are seeking parallel lines. The more non-parallel the lines, the more unlikely an additive model is a proper fit. If the lines are clearly not parallel, then there is interaction between the factors and an interaction term is probably needed in the model. If we chose to add an interaction component to our model, we would simply augment the formula by adding as.factor(Year):Season:\n\nanova(lm(Value ~ Season + as.factor(Year) + as.factor(Year):Season, datr.long))\n\nAnalysis of Variance Table\n\nResponse: Value\n                       Df  Sum Sq Mean Sq  F value                Pr(>F)    \nSeason                  3  2.6276 0.87588  86.9506 < 0.00000000000000022 ***\nas.factor(Year)         5 14.9474 2.98948 296.7720 < 0.00000000000000022 ***\nSeason:as.factor(Year) 15  0.6324 0.04216   4.1856            0.00007479 ***\nResiduals              48  0.4835 0.01007                                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe p-value associated with the interactive term as.factor(Year):Season is small suggesting that the interaction should not be ignored. Note that had the two-way ANOVA been run on the replicate-free version of the data (dat.long), the code chunk would not have generated p-values.\nCoefficients can be extracted using the coefficients() function.\n\ncoefficients(lm(Value ~ Season + as.factor(Year) + as.factor(Year):Season, datr.long))\n\n                     (Intercept)                     SeasonSpring                     SeasonSummer \n                     1.514706536                     -0.219142681                      0.509122788 \n                    SeasonWinter              as.factor(Year)1972              as.factor(Year)1973 \n                    -0.089699659                      0.558927966                      0.705495273 \n             as.factor(Year)1974              as.factor(Year)1975              as.factor(Year)1976 \n                     0.896043592                      1.169276817                      1.564987568 \nSeasonSpring:as.factor(Year)1972 SeasonSummer:as.factor(Year)1972 SeasonWinter:as.factor(Year)1972 \n                    -0.055451015                     -0.241938153                     -0.271609111 \nSeasonSpring:as.factor(Year)1973 SeasonSummer:as.factor(Year)1973 SeasonWinter:as.factor(Year)1973 \n                     0.131119255                     -0.206021571                     -0.007361442 \nSeasonSpring:as.factor(Year)1974 SeasonSummer:as.factor(Year)1974 SeasonWinter:as.factor(Year)1974 \n                    -0.119064438                     -0.247193634                      0.090749725 \nSeasonSpring:as.factor(Year)1975 SeasonSummer:as.factor(Year)1975 SeasonWinter:as.factor(Year)1975 \n                     0.050233770                     -0.333834613                     -0.129534625 \nSeasonSpring:as.factor(Year)1976 SeasonSummer:as.factor(Year)1976 SeasonWinter:as.factor(Year)1976 \n                     0.165218848                     -0.386425892                     -0.317247156 \n\n\nNote that one disadvantage in having so many terms in the model is that you reduce the power of the model. In this example we now have 23 variables and a little more than three times as many measurements (72 to be exact). It’s always good practice to seek the most parsimonious model possible. This may involve, for example, grouping factor levels if theory supports it."
  },
  {
    "objectID": "ANOVA.html#extracting-effects-for-each-level",
    "href": "ANOVA.html#extracting-effects-for-each-level",
    "title": "ANOVA",
    "section": "6.2 Extracting effects for each level",
    "text": "6.2 Extracting effects for each level\nThe regression model will wrap one level’s effect from each factor into the intercept term however, you might wish to know each level’s effect on the measured variable. This requires decomposing the model’s intercept into an overall mean measurement and two level effects (one for each factor). This can be done by running the ANOVA analysis using aov() then wrapping its output with the model.tables() function as follows:\n\nM1 <- aov(Value ~ Season + as.factor(Year), dat.long)\nmodel.tables(M1)\n\nTables of effects\n\n Season \nSeason\n    Fall   Spring   Summer   Winter \n 0.03958 -0.15542  0.27625 -0.16042 \n\n as.factor(Year) \nas.factor(Year)\n   1971    1972    1973    1974    1975    1976 \n-0.7404 -0.3254 -0.0704  0.0971  0.3271  0.7121 \n\n\nThe overall mean (and new model intercept) can be extracted from the original table as follows:\n\nmean(dat.long$Value)\n\n[1] 2.300417\n\n\nWhile this approach expands the model by adding two more variables, it does allow one to quantify the effect each level has in explaining the coliform counts. For example, the model suggests that the overall mean (log) coliform count is 2.3 but the Spring seasons lower that value by about -0.155. Likewise, we can state that the year 1975 increases the overall mean by about 0.327. It’s also clear from the output that 1971 had the greatest effect on coliform count by dropping the overall value by -0.7404.\nNote, however, that if the model has a significant cross-factor interaction effect, you should not attempt to draw anything too meaningful from the main level effects. In such a scenario, the most meaningful result to glean from the model is the interaction between the factors."
  },
  {
    "objectID": "ChiSquare_test.html",
    "href": "ChiSquare_test.html",
    "title": "Comparing frequencies: Chi-Square tests",
    "section": "",
    "text": "Last modified on 2023-04-06\nPackages used in this tutorial:"
  },
  {
    "objectID": "ChiSquare_test.html#example",
    "href": "ChiSquare_test.html#example",
    "title": "Comparing frequencies: Chi-Square tests",
    "section": "2.1 Example",
    "text": "2.1 Example\n\n2.1.1 Problem\nIf a survey of 200 individuals is conducted where the respondents are asked to select one of five answers, one might want to determine if all answers have an equal chance of being chosen, in other words, does each answer get selected about one fifth of the time (one out of five possible answers). So out of 200 respondents, we would expect to see each answer selected about 40 times given our null hypothesis that each answer had an equal chance of being selected. The observed and expected values can be summarized in a frequency table:\n\n\n\n\nObserved frequency\nExpected frequency\n\n\n\n\nAnswer 1\n36\n40\n\n\nAnswer 2\n44\n40\n\n\nAnswer 3\n38\n40\n\n\nAnswer 4\n37\n40\n\n\nAnswer 5\n45\n40\n\n\n\n——————–\n——————-\n\n\n\n200\n200\n\n\n\nNote that the totals in both the observed and expected columns should equal 200 (the number of respondents in this sample).\n\n\n2.1.2 Solution\nThe \\(\\pmb\\chi^2\\)-statistic is the sum of the squared difference between observed and expected counts divided by the expected frequency, or, \\[\n\\chi^2 = \\sum{\\frac{(observed frequency - expected frequency)^2}{expected frequency}}\n\\] Computing \\(\\chi^2\\) for our data we get: \\[\n\\chi^2 = \\frac{(36-40)^2}{40} + \\frac{(44-40)^2}{40} + \\frac{(38-40)^2}{40} + \\frac{(37-40)^2}{40} + \\frac{(45-40)^2}{40} = 1.75\n\\]\nNext, we need to see where our \\(\\chi^2\\) value lies on a \\(\\pmb\\chi^2\\)-curve. The shape of the \\(\\chi^2\\) curve is defined by the degrees of freedom, \\(df\\) (this is analogous to the Student curve). \\(df\\) is computed from the number of possible outcomes minus one or \\(df = 5 -1 = 4\\) for our working example. The shape of the \\(\\chi^2\\) curve for 5 \\(df\\) ’s and the placement of our \\(\\chi^2\\) statistic on that curve are shown below:\n\n\n\n\n\nThe area highlighted in light red to the right of the \\(\\chi^2\\) statistic shows the probability of a \\(\\chi^2\\) value more extreme than the one observed. In other words, if the expected outcome was true (i.e. all answers having equal probability of being selected) the probability of coming up with a \\(\\chi^2\\) more extreme than ours is 78%. Therefore we have a difficult time rejecting the null hypothesis and conclude that our observed frequencies are consistent with our null hypothesis and that any variability can be explained by chance alone.\nThe \\(\\chi^2\\) can be easily implemented in R using the chisq.test() function.\n\nobs <- c(36, 44, 38, 37, 45)\nexp <- c(.20, .20, .20, .20, .20)\nchisq.test(obs, p=exp)\n\nThe output of the chisq.test function looks like this:\n\n\n\n    Chi-squared test for given probabilities\n\ndata:  obs\nX-squared = 1.75, df = 4, p-value = 0.7816\n\n\nThe expected frequency values stored in the variable exp must be presented as fractions and not counts. Since all expected frequencies are equal, they all take on the fraction value of \\(40/200 = 0.20\\). The sum of the expected fraction must be 1 or R will return an error message. This ends our example."
  },
  {
    "objectID": "ChiSquare_test.html#the-chi2-curve-explained",
    "href": "ChiSquare_test.html#the-chi2-curve-explained",
    "title": "Comparing frequencies: Chi-Square tests",
    "section": "2.2 The \\(\\chi^2\\) curve explained",
    "text": "2.2 The \\(\\chi^2\\) curve explained\nTo understand what the \\(\\chi^2\\) curve represents, we will run a simulation where one of the 5 answers is selected at random (assuming that all answers have equal probability of being picked as defined by \\(H_o\\)). Think of the simulation as consisting of 999 investigators who each survey 200 individuals at random. Also assume that we know that each answer has equal probability of being picked; this implies that only chance variability will generate counts of answers slightly off from what would be expected.\nFor each of the the 999 simulated samples, we compute \\(\\chi^2\\) as was done in the previous example. We end up with 999 \\(\\chi^2\\) values which we then plot using a histogram.\n\nn     <- 999                    # Number of times to collect a sample\nsize  <- 200                    # Sample size (i.e. number of respondents)\nexp   <- c(.2, .2, .2, .2, .2)  # Expected fractions\nchi.t <- vector(length = n)     # An empty vector that will store the Chi-sq values\n\nfor (i in 1:n){\n  survey.results <- sample( c(1,2,3,4,5), size = size, replace = TRUE)\n  survey.sum     <- table(survey.results)\n  chi.t[i]       <- chisq.test(survey.sum, p=exp)$statistic\n}\n\nhist(chi.t, breaks=20)\n\nThis simulation consists of a for loop (think of each iteration of the loop as representing the survey results from one of the 999 investigators). For each iteration, one answer (out of five, each identified as c(1,2,3,4,5) in the code) is selected at random 200 times (this is what the sample() function does). The results are tallied in the variable survey.results. The table() function tallies the frequency of each selected answer. The chisq.test() function computes the \\(\\chi^2\\) test and the $statistic parameter extracts the \\(\\chi^2\\) statistic from the chisq.test() result. This statistic is tallied, resulting in 999 \\(\\chi^2\\) values, which are then past to the hist() plotting function. The resulting histogram should look something like this:\n\n\n\n\n\nThe shape of the histogram is very close to the one defined by the \\(\\chi^2\\) curve for 4 \\(df\\) (displayed as a red curve below)."
  },
  {
    "objectID": "ChiSquare_test.html#can-the-chi2-value-be-too-good",
    "href": "ChiSquare_test.html#can-the-chi2-value-be-too-good",
    "title": "Comparing frequencies: Chi-Square tests",
    "section": "2.3 Can the \\(\\chi^2\\) value be ‘too’ good?",
    "text": "2.3 Can the \\(\\chi^2\\) value be ‘too’ good?\nYou’ll note that the \\(\\chi^2\\) curve is positively skewed (and strongly so when the \\(df\\) is small), hence the probability of computing a very small \\(\\chi^2\\) value diminishes precipitously when the test statistic approaches 0. The following graph shows the bottom and top 10% probability regions.\n\n\n\n\n\nGregor Mendel is credited with having set the ground work for modern day genetics by explaining heredity using garden peas in the mid 1800’s. He bred a pure yellow strain of peas and a pure green strain of peas. He then cross-pollinated the two colored peas to generate a 1st generation of hybrid peas. The result was a batch of all yellow peas (i.e. no visible trace of the green parent pea). Mendel then cross-pollinated the 1st generation peas to produce a second generation of peas. This second batch produced both green and yellow peas (about 25% green and 75% yellow).\nIn addition to color, Mendel also studied the physical characteristics of the peas, noting that peas were either smooth or wrinkled. Following the same experimental setup as that for the colored peas, Mendel noted that the second generation of peas produced roughly 25% wrinkled peas and 75% smooth peas.\nOne of his trials which mixed pea color and texture produced the following outcome (Freedman et al., p 470):\n\n\n\nType of pea\nObserved number\n\n\n\n\nSmooth yellow\n315\n\n\nWrinkled yellow\n101\n\n\nSmooth green\n108\n\n\nWrinkled green\n32\n\n\n\n—————\n\n\n\nsum = 556\n\n\n\nGiven the theory (based on the recessive/dominant nature of genes) that 75% of the peas would be yellow and that 75% of the peas would be smooth, we can come up with expected outcomes based on the following probability table (\\(y\\) = yellow, \\(g\\) = green, \\(s\\) = smooth and \\(w\\) = wrinkled):\n\n\n\nColor\nTexture\nProbability\n\n\n\n\ny\ns\n0.75 * 0.75 = 0.5625\n\n\ny\nw\n0.75 * 0.25 = 0.1875\n\n\ng\ns\n0.25 * 0.75 = 0.1875\n\n\ng\nw\n0.25 * 0.25 = 0.0625\n\n\n\nSo, out of 556 peas, we would expect \\(0.5625 \\times 556 = 313\\) peas having a \\(y\\)/\\(s\\) combination. Likewise, we would expect \\(0.1875 \\times 556 = 104\\) peas having a \\(y\\)/\\(w\\) combination. We can compute the other two probabilities and create the following frequency table:\n\n\n\nType of pea\nObserved number\nExpected\n\n\n\n\nSmooth yellow\n315\n313\n\n\nWrinkled yellow\n101\n104\n\n\nSmooth green\n108\n104\n\n\nWrinkled green\n32\n35\n\n\n\n—————–\n——–\n\n\n\n556\n556\n\n\n\nIn the early 1900’s, the statistician Ronald Fisher was skeptical of Mendel’s experimental results. He used a \\(\\chi^2\\) test to prove the point. The \\(\\chi^2\\)-statistic for the color/texture experiment can be computed in R as follows:\n\nobs <- c(315, 101, 108, 32)\nexp <- c(0.5625, 0.1875, 0.1875, 0.0625) \nchisq.test(obs, p = exp)\n\n\n    Chi-squared test for given probabilities\n\ndata:  obs\nX-squared = 0.47002, df = 3, p-value = 0.9254\n\n\nBy default, chisq.test’s probability is given for the area to the right of the test statistic. Fisher was concerned with how well the observed data agreed with the expected values suggesting bias in the experimental setup. So we want to know how likely we are to calculate a \\(\\chi^2\\) smaller than what would be expected by chance variation alone. The area of interest is highlighted in red in the following figure:\n\n\n\n\n\nThere is a 7.5% chance (\\(1 - 0.9254 = 0.075\\)) that an observed \\(\\chi^2\\) would be smaller (i.e. one that is in even better agreement with the expect value) than the one observed if chance variability alone were to explain the difference. To Fisher, this small probability suggested that Mendel’s experimental setup may have been somewhat biased (but it must be noted the Mendel’s theory is sound and has been proven many times in separate experiments, but not with \\(\\chi^2\\) probabilities as good as Mendel’s)."
  },
  {
    "objectID": "ChiSquare_test.html#example-1",
    "href": "ChiSquare_test.html#example-1",
    "title": "Comparing frequencies: Chi-Square tests",
    "section": "3.1 Example",
    "text": "3.1 Example\n\n3.1.1 Problem\nWere passengers/crew members of different class status (i.e. 1st, 2nd, 3rd or crew) equally likely to perish in the Titanic?\n\n\n\n\nPerished\nSurvived\n\n\n\n\n1st\n122\n203\n\n\n2nd\n167\n118\n\n\n3rd\n528\n178\n\n\ncrew\n673\n212\n\n\n\n\n\n3.1.2 Solution\nThis problem is a test of independence where we are testing whether the observed categorical counts are consistent with what we would expect if \\(H_o\\) (i.e. all classes of passengers/crew members had equal chance of perishing) was true.\nThe first step is to create what is a called a contingency table where we sum all rows and columns\n\n\n\n\nPerished\nSurvived\nRow sums\n\n\n\n\n1st\n122\n203\n325\n\n\n2nd\n167\n118\n285\n\n\n3rd\n528\n178\n706\n\n\ncrew\n673\n212\n885\n\n\n\n———-\n———-\n\n\n\n\n1490\n711\n\n\n\n\nSumming either the row sums or column sums gives us a total number of souls on the Titanic of 2201. This will be the value \\(n\\) in the next steps.\nNext, we will compute the expected counts assuming that all classes of passenger and crew members had an equal probability of perishing. The formula to compute the expectation is simple. It’s the product of the row sum and column sum divided by the total number of souls. For example, for \\(row\\; 1\\)/\\(col\\; 1\\) (i.e. the total number of 1st class passengers that perished), the expected count assuming \\(H_o\\) is at play here is, \\[\nE(n_{11}) = \\frac{r_1c_1}{n} = \\frac{(325)(1490)}{2201} = 220\n\\] where \\(N_{11}\\) refers to cell \\(row\\; 1\\)/\\(col\\; 1\\). Likewise, we can compute the expected value for \\(row\\; 1\\)/\\(col\\; 2\\) as follows, \\[\nE(n_{12}) = \\frac{r_1c_2}{n} = \\frac{(325)(711)}{2201} = 105\n\\] The above procedure can be repeated for the remaining six cells giving the expected table:\n\n\n\n\nPerished\nSurvived\n\n\n\n\n1st\n220\n105\n\n\n2nd\n193\n92\n\n\n3rd\n478\n228\n\n\ncrew\n599\n286\n\n\n\nNote that if you round the numbers (this is not required, you can work with decimal values as well), it is good practice to check that the total count adds up to \\(n\\) (i.e. 2201 in our working example).\nThe next step is to compute the \\(\\chi^2\\) statistic between the observed and expected values. \\[\n\\chi^2 = \\frac{(122 - 220)^2}{220} + \\frac{(203-105)^2}{105} + ... + \\frac{(212-286)^2}{286} = 190.4\n\\]\nThe shape of the \\(\\chi^2\\) curve is determined by the degrees of freedom, \\(df\\). For a two-factor analysis, \\(df\\) is the product of the number of rows minus one and the number of columns minus one or, \\[\ndf = (rows - 1)(columns - 1)\n\\]\nFor our working example, the \\(df\\) is \\((4-1)(2-1)=3\\). Our observed \\(\\chi^2\\) value falls to the very far right side of the \\(\\chi^2\\) curve. It’s clear that the probability of coming up with a \\(\\chi^2\\) value as extreme as ours is nearly 0, in other words it’s very likely that the observed discrepancy in survival rates between classes and crew members is not due to chance variability.\n\n\n\n\n\nThe \\(\\chi^2\\) value (and associated \\(P\\) value) can be easily computed in R as demonstrated in the following blocks of code. First, we create the data frame that will store the counts.\n\n# Create the data frame\ndat <- data.frame(Perished  = c(122,167,528,673), \n                  Survived  = c(203,118,178,212), \n                  row.names = c(\"1st\", \"2nd\", \"3rd\", \"crew\"))\n\nThe data frame can be viewed by calling the data frame dat,\n\ndat\n\n     Perished Survived\n1st       122      203\n2nd       167      118\n3rd       528      178\ncrew      673      212\n\n\nThe \\(\\chi^2\\) test can then be computed as follows:\n\nchisq.test(dat)\n\n\n    Pearson's Chi-squared test\n\ndata:  dat\nX-squared = 190.4, df = 3, p-value < 0.00000000000000022\n\n\nIf you want to see the expected count table along with additional contingency table data, you can store the output of the chisq.test() to a variable, then extract additional information from the function.\n\nchi.t <- chisq.test(dat)\n\nThe expected values can be extract from the variable chi.t as follows:\n\nchi.t$expected\n\n     Perished  Survived\n1st  220.0136 104.98637\n2nd  192.9350  92.06497\n3rd  477.9373 228.06270\ncrew 599.1140 285.88596\n\n\nWe can also visualize the frequencies between expected and observed using the mosaicplot() function.\n\nOP <- par(mfrow=c(1,2), \"mar\"=c(1,1,3,1))\nmosaicplot(chi.t$observed, cex.axis =1 , main = \"Observed counts\")\nmosaicplot(chi.t$expected, cex.axis =1 , main = \"Expected counts\\n(if class had no influence)\")\npar(OP)\n\n\n\n\nThe polygons are proportional to the count values. Note how many more 1st class passengers survived the sinking of the Titanic then what would have been expected had class not played a role in who perished and who survived."
  },
  {
    "objectID": "ChiSquare_test.html#computing-confidence-intervals-for-variances",
    "href": "ChiSquare_test.html#computing-confidence-intervals-for-variances",
    "title": "Comparing frequencies: Chi-Square tests",
    "section": "5.1 Computing confidence intervals for variances",
    "text": "5.1 Computing confidence intervals for variances\n\n5.1.1 Example\nIf a sample of size 100 has a standard deviation, \\(s\\), of 10.95. What is the standard deviation’s confidence interval for an \\(\\alpha\\) of 0.05?\n\n\n5.1.2 Solution\nTo compute the \\(\\chi^2\\)-statistics that will define the confidence interval, \\(CI\\), we need to identify the probabilities (\\(P\\)-values) that define the ‘rejection’ regions of the \\(\\chi^2\\) curve. Given that we can compute the degrees of freedom (100 -1 = 99) and that we are given an \\(\\alpha\\) value of 0.05, we can draw the \\(\\chi^2\\) curve and identify the ‘rejection’ regions. This problem can be treated as a two-tailed test with a lower \\(P\\) value of \\(0.05/2=0.025\\) and an upper \\(P\\) value of \\(1 - 0.05/2=0.975\\), however, unlike a hypothesis test where we seek one \\(\\chi^2\\) value, we are looking for two \\(\\chi^2\\) values.\n\n\n\n\n\nThe two \\(\\chi^2\\) values that define the interval can now be computed using the qchisq() function.\n\nqchisq(p = 0.025, df = 99)\n\n[1] 73.36108\n\nqchisq(p = 0.975, df = 99)\n\n[1] 128.422\n\n\nNow that we have our \\(\\chi^2\\) values, we can find the \\(\\sigma^2\\) values (and by extension the standard deviation \\(\\sigma\\)) that define the \\(CI\\). We simply solve the \\(\\chi^2\\) equation for \\(\\sigma^2\\): \\[\n\\sigma^2 = \\frac{(n-1)s^2}{\\chi^2}\n\\]\nThe confidence interval \\(CI\\) for the population variance is thus: \\[\n\\frac{(n-1)s^2}{\\chi_{0.925}^2} < \\sigma^2 < \\frac{(n-1)s^2}{\\chi_{0.025}^2}\n\\] or \\[\n\\frac{(99)10.95^2}{128.4} < \\sigma^2 <  \\frac{(99)10.95^2}{73.36}\n\\] giving us \\[\n92.4 < \\sigma^2 <  161.8\n\\]\nand the confidence interval for the population standard deviation, \\(\\sigma\\) is: \\[\n\\sqrt{92.4} < \\sigma < \\sqrt{161.8}\n\\] or \\[\n9.6 < \\sigma <  12.72\n\\]"
  },
  {
    "objectID": "ChiSquare_test.html#test-hypotheses-on-population-variances",
    "href": "ChiSquare_test.html#test-hypotheses-on-population-variances",
    "title": "Comparing frequencies: Chi-Square tests",
    "section": "5.2 Test hypotheses on population variances",
    "text": "5.2 Test hypotheses on population variances\n\n5.2.1 Example\nA machine shop is manufacturing a part whose width must be 19”. The customer requires that the width have a standard deviation no greater than 2.0 \\(\\mu m\\) with a confidence of 95%. 15 parts are sampled at random and their widths are measured. The sample standard deviation, \\(s\\), is 1.7 \\(\\mu m\\). Is the standard deviation for all the parts, \\(\\sigma\\), less than 2.0? (i.e. given that the sample’s \\(s\\) is susceptible to natural variability about its true value, can we be confident that the true population \\(\\sigma\\) is less than 2.0)\n\n\n5.2.2 Solution\nThe question asks that we test the null hypothesis, \\(H_o\\), that the standard deviation from the population, \\(\\sigma_o\\), is less than \\(2.0\\). The sample standard deviation, \\(s\\), is 1.7. We need to test whether or not the difference between \\(s\\) and \\(\\sigma_o\\) is do to chance variability or if the difference is real. Since we will be using the \\(\\chi^2\\) test, we will need to work with variances and not standard deviations, so \\(H_o\\) must be stated in terms of \\(\\sigma_o^2\\) and not \\(\\sigma_o\\). Our test statistic is therefore computed as follows:\n\\[\n\\chi^2 = \\frac{(n-1)s^2}{\\sigma_o^2} = \\frac{(15-1)1.7^2}{2.0^2} = 10.1\n\\]\nThe probability of getting a \\(\\chi^2\\) of 10.1 is 0.246 (or 24.6%). So if chance variability alone were to explain the difference between our observed \\(\\chi^2\\) value and the hypothesized \\(\\chi^2\\) value associated with \\(\\sigma_o^2\\), there would be a 24.6% chance of getting a \\(\\chi^2\\) as extreme as ours. Now the customer wants to be 95% confident that the difference between our observed \\(s\\) and the threshold \\(\\sigma_o\\) of 2.0 is real (i.e. that it’s less than 2.0) and not due to chance variability. This is tantamount to a one-sided hypothesis test where \\[\nH_o: \\sigma^2 = 2.0^2\n\\] \\[\nH_a: \\sigma^2 < 2.0^2\n\\]\nwhere \\(\\sigma\\) is the standard deviation of the width for all parts being manufactured. The customer wants to be 95% confident that \\(\\sigma\\) is less than 2.0. This translates to having an observed \\(P\\) closer to the left tail of the distribution. The exact cutoff is 95% from the right-hand side, or 5% from the left hand side (dashed line in the following graph). Our observed \\(P\\) value of 0.246 is greater than the desired \\(\\alpha\\) value of 0.05 meaning that there is a good chance that our observed difference in width variance is due to chance variability (at least at the 5% confidence level). We therefore cannot reject the null and must inform the customer that the machined parts do not meet the desired criteria.\n\n\n\n\n\nThe test can easily be implemented in R as follows:\n\nchi.t <- (15 - 1) * 1.7^2 / 2.0^2\npchisq(chi.t, 15-1)\n\n[1] 0.2462718\n\n\nwhere the pchisq() function returns the probability for our observed \\(\\chi^2\\) value with a \\(df\\) of \\((15-1)\\)."
  },
  {
    "objectID": "CI.html",
    "href": "CI.html",
    "title": "Confidence intervals",
    "section": "",
    "text": "Last modified on 2023-04-06\n\n\n\n\n\n\n\n1 Introduction\nMost data, or batches, are subsets of some underlying population. From these batches, we attempt to infer about the state of the population. For example, we may want to determine how many hours of TV are watched in each household each week. Since we seldom have the resources needed to collect the data for all households, we opt to collect data for a small subset of the population. From this sampled survey, we compute a summary statistic (such as the mean hours of TV watched each week). We then use this sample statistic as an estimate of the number of hours watched by all households in the population.\nNow, if another investigator were to sample other households at random, she will probably come up with a slightly different summary statistic. Likewise, if a hundred other investigators were to sample households at random, they would come up with a hundred slightly different means of TV hours watched. Let’s explore this idea with a simulated data set:\n\nnum.samp   <- 1000                      # Number of samples to collect\nsamp.size  <- 50                        # Size of each sample\nsamp.mean  <- vector(length = num.samp) # Create an empty vector array\n\n# Sample the population 'num.samp' of times then compute the sample mean\nfor (i in 1:num.samp){\n  samp.mean[i] <- mean(rbeta(samp.size,20,20,ncp=0)*60)\n}\n\nIn the above code, the first two lines define the number of samples to collect, 1000, and the number of households to survey in each sample, 50. In other words, we have 1000 investigators each sampling 50 households. The for loop collects a new sample (of 50 households) at each iteration 1000 times. The function rbeta(samp.size,20,20,ncp=0)*60 randomly generates 50 values from a predefined distribution (think of this as simulating the number of hours of TV watched in each household 50 times). The sample mean of TV hours watched for each 50 household sample is then calculated using the mean() function. So each investigator computes a single value that represents the mean hours of TV watched in the 50 households. Since we have 1000 investigators, we have 1000 sample means. We can plot the distribution of the sample means using a histogram.\n\nhist(samp.mean, xlab=\"Mean TV hours per week\")\n\n\n\n\n\n\nNote that the distribution of the underlying population mean need not be normally distributed to produce a normally distributed sample mean distribution. That said, the histogram just presented does not represent the distribution of mean hours of TV watched for the population, but the distribution of the sample averages.\n\n\n2 Estimate of a population statistic from many samples\nFrom the histogram it seems that, out of the 1000 samples, most sample means are centered around 30. In fact, we can compute the arithmetic mean from the 1000 samples. This statistic is referred to as the grand mean (not to be confused with each individual sample mean of which there are 1000 in our example).\n\ngrand.mean <- mean(samp.mean)\ngrand.mean\n\n[1] 30.01377\n\n\nFrom the grand mean, we might be tempted to infer that the mean hours of TV watched by each household in the entire population is 30.01 hours. But note from the histogram that there is some variability in the means computed from each 50 household samples. It might behoove us to assess the chance that this estimate (the grand mean) is not representative of the whole population mean. We can find out how big the chance error might be by calculating the standard error.\nThe standard error is the standard deviation of the sampling distribution of a statistic; it’s the likely size of chance error when estimating a statistic. In our case the standard error of our sample means, \\(SE\\), is computed by taking the standard deviation of the 1000 sample means:\n\nSE <- sd(samp.mean)\nSE\n\n[1] 0.6566372\n\n\nWe can now state that the average number of hours of TV watched per household per week is 30.01 \\(\\pm\\) 0.66. It’s important to note that \\(SE\\) is not an estimate of the population standard deviation but a measure of uncertainty in the population mean estimate.\nIn most cases, we do not have the luxury of collecting hundreds or thousands of samples. We usually only have a single sample to work with (hence a single sample from which to make inferences about the whole population). Methods in estimating the sample variability given a single sample is covered next.\n\n\n3 Estimate of a population statistic from a single sample\nLet’s assume that we only have a single sample of 50 households to work with. Let’s store these values in a vector we’ll call x.\n\nx <- c(25.7, 38.5, 29.3, 25.1, 30.6, 34.6, 30.0, 39.0, 33.7, 31.6, \n       25.9, 34.4, 26.9, 23.0, 31.1, 29.3, 34.5, 35.1, 31.2, 33.2, \n       30.2, 36.4, 37.5, 27.6, 24.6, 23.9, 27.0, 29.5, 30.1, 29.6, \n       27.3, 31.2, 32.5, 25.7, 30.1, 24.2, 24.1, 26.4, 31.0, 20.7, \n       33.5, 32.2, 34.7, 32.6, 33.5, 32.7, 25.6, 31.1, 32.9, 25.9)\n\nThe standard error of the mean from the one sample, can be estimated using the following formula:\n\\[\nSE_{mean} = \\frac{SD}{\\sqrt{sample\\; size}}\n\\]\nwhere \\(SD\\) is the standard deviation of number of hours watched for the entire population (i.e. all households, not just those sampled). The standard error of the mean is sometimes represented as \\(\\sigma_{\\bar X}\\). It’s important to note two things here: * this approximation applies only to the uncertainty associated with the estimate of the population mean (and not other statistics like the median, count or percentage–these are treated later), * this approximation holds for a large sample sizes only (usually 30 or greater).\nHowever, there’s a problem. We usually don’t know the population’s \\(SD\\) (if we did, we would not need to bother with sampling in the first place!) so, for a large sample size, we can estimate \\(SD\\) using the sample means’ standard deviation, \\(SD_{sample}\\) giving us \\[\nSE_{mean} = \\frac{SD_{sample}}{\\sqrt{sample\\; size}}.\n\\]\nSo following up with our single sample example, we can estimate the population mean and the standard error of the mean from our sample as:\n\nmean.x <- mean(x)\nSE.x   <- sd(x) / sqrt(length(x))\n\nwhere length(x) is an R function that gives us the number of values in the vector x (i.e. our sample size \\(n\\)). In this example, the population mean estimate is 30.14 hours with a standard error, \\(\\sigma_{\\bar X}\\), of 0.6 hours.\nSo how exactly do we interpret \\(SE\\) ? As noted earlier, \\(SE\\) is not an estimate of the population standard deviation. It’s a measure of how likely the interval, defined by \\(SE\\), encompasses the true (population) mean. In the following figure, the sample means for 30 samples are plotted along with error bars depicting their \\(\\pm\\) 1 \\(SE\\) (another way to interpret a \\(SE\\) is to say that we are ~68% confident that the \\(\\pm\\) 1 \\(SE\\) interval encompasses the true population mean). The blue vertical line represents the true population mean of 30, the expected value (the unknown parameter we are usually seeking). Batches of data (samples) whose \\(\\pm\\) 1 \\(SE\\) range does not contain the true population mean are plotted in red.\n\n\n\n\n\nIt’s clear from the figure that 12 samples have a 68% confidence interval that do not contain the true population mean. If we want to increase our confidence that the sample mean contain the true mean value, we can widen our confidence from 1 \\(SE\\) to 2 \\(SE\\) which covers about 95% of each sample distribution.\n\n\n\n\n\nNow, only three of the 30 samples have a confidence interval that does not contain the true population mean of 30 hours of TV watched per week.\nOne \\(SE\\) on both sides of the sample mean encompasses a probability of about 68% (34% on either side of the mean). This is our 68% confidence interval (i.e. there is a 68% chance that this interval contains the true population mean). If we want to increase our confidence that the interval contains the population mean, we can widen it by say 2 \\(SE\\) which provides us with about 95% confidence.\nThe following figure displays the histogram from 1000 sample means superimposed with a Gaussian curve. The (red) shaded areas represent the fraction of the sample mean values that fall within each SE ranges. Note that fewer and fewer sample means fall within SE intervals as the sample means diverge from the grand mean.\n\n\n\n\n\nSo, given our most recent example, we can state that there is a ~ 68% chance that the interval (30.14 - 0.6) and (30.14 + 0.6) contains the true population mean of hours of TV watched each week or, put more succinctly, there is a 68% chance that the mean hours of TV watched by the population is 30.14 \\(\\pm\\) 0.6.\nNote that we are making a statement about the probability that the interval includes the population mean and not the probability that the population mean falls between this interval. The latter would imply that the population mean is a random variable which it’s not, it’s static!\nThe standard error equation used thus far only applies to the mean, if another population statistic is to be estimated, then other \\(SE\\) formulas have to be used. Other types of \\(SE\\)’s include the standard error of sum of samples, and fractions (of a binary output) of samples (Freedman et al., p.422):\n\\[\nSE_{sum} = \\sqrt{sample\\; size} \\times SD\n\\]\n\\[\nSE_{fraction} = \\frac{\\sqrt{(fraction\\; of\\; 1's)(fraction\\; of\\; 0's)}}{\\sqrt{sample\\; size}}\n\\]\n\n\n4 A universal approach to computing the standard error: the bootstrap\nThe preceding section highlighted a few formulae for SE and some restricting assumptions such as the sample size. What if we want to to estimate the population median, mode, etc.. for which a formula may be illusive or non-existent? The solution to this problem is a technique called bootstrap. A bootstrap is a computer simulation often used to compute measures of accuracy to statistical estimates. The idea is simple: re-sample from the sample many times, then calculate the statistic for each sub-sample. The re-sampling is usually done with replacement (i.e. a same value can be picked more than once). The bootstrap standard estimated error is the standard deviation of the bootstrap samples.\nContinuing with the 50 household sample, we can apply the bootstrap technique to estimate the population median in R as follows:\n\nmedian.boot <- numeric()   # Create an empty vector\nfor (i in 1:10000){\n  sample.boot    <- sample(x, size = length(x), replace=TRUE)\n  median.boot[i] <- median(sample.boot)\n}\nSE.boot <- sd(median.boot)\nSE.boot\n\n[1] 0.6965248\n\n\nIn this example, the sample x is re-sampled 10,000 times (each sample having a size equal to the original sample, length(x)). The function sample() samples anew from the sample x with replacement each time. The median of each re-sampled batch is stored in the variable median.boot[i]. The variable SE.boot stores the standard error for the bootstrap sample medians, or 0.6965248 in this working example (note that your result may differ slightly given that the re-sampling process is random).\nAnother way to implement the bootstrap technique is to use the function boot() from the boot library. This reduces the code to:\n\nlibrary(boot)\nf.med       <- function(Y,i) median(Y[i])\nmedian.boot <- boot(x, R = 10000, statistic = f.med)\nSE.boot     <- sd( as.vector(median.boot$t))\nSE.boot\n\n[1] 0.690165\n\n\nIn this example, we are defining a function called f.med where the function returns the median value for an array. This is a simple example of a function, but this demonstrates how one can create a customized statistic for which \\(SE\\) is sought. The function boot returns a list and not a numeric vector. To view the contents of a list, you can invoke the structure function str().\n\nstr(median.boot)\n\nList of 11\n $ t0       : num 30.4\n $ t        : num [1:10000, 1] 31.6 30.1 31.1 30.1 31.2 ...\n $ R        : num 10000\n $ data     : num [1:50] 25.7 38.5 29.3 25.1 30.6 34.6 30 39 33.7 31.6 ...\n $ seed     : int [1:626] 10403 593 -1213961628 1450265616 -1650302568 -252474396 -577513603 -474387378 1770380790 -799506326 ...\n $ statistic:function (Y, i)  \n  ..- attr(*, \"srcref\")= 'srcref' int [1:8] 2 16 2 41 16 41 2 2\n  .. ..- attr(*, \"srcfile\")=Classes 'srcfilecopy', 'srcfile' <environment: 0x00000188b27fd050> \n $ sim      : chr \"ordinary\"\n $ call     : language boot(data = x, statistic = f.med, R = 10000)\n $ stype    : chr \"i\"\n $ strata   : num [1:50] 1 1 1 1 1 1 1 1 1 1 ...\n $ weights  : num [1:50] 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 ...\n - attr(*, \"class\")= chr \"boot\"\n - attr(*, \"boot_type\")= chr \"boot\"\n\n\nThe list contains close to a dozen components. The component of interest to us is t which stores the median values from all 10,000 sub-samples. This component is accessed by adding the $ symbol to the end of the list name median.boot (i.e. median.boot$t). If you type median.boot$t at a command prompt, you will note that the data are stored as a matrix (and not a vector). Hence, if you want to compute the standard deviation of this matrix by invoking sd(median.boot$t), you will be presented with a warning along the lines of Warning: sd(<matrix>) is deprecated.... To circumvent this warning, you can convert the matrix to a vector using the as.vector() function (i.e. sd( as.vector(meadian.boot$t) ).\n\n\n5 Confidence intervals\nThe idea of a confidence interval, CI, is a natural extension of the standard error. It allows us to define a level of confidence in our population parameter estimate gleaned from a sample. For example, if we wanted to be 95% confident that the range of mean TV hours per week computed from our sample encompasses the true mean value for all households in the population we would compute this interval by adding and subtracting \\(1.96\\; SE\\) to/from the sample mean. But beware, people will sometimes state this as “… there is a 95% chance that the population mean falls between such and such values…” which is problematic as noted earlier since it implies that the population mean is a random variable when in fact it’s not. The confidence interval reminds us that the chances are in the sampling and not the population parameter.\nA confidence interval of 68% and 95% are easily estimated from \\(1 SE\\) or \\(1.96 SE\\) respectively, but what if we want to define some other confidence interval such as 85% or 90%? To estimate the confidence interval for any other value, simply invoke the Student’s t quantile function qt() in conjunction with \\(SE\\). For example, to generate a 90% confidence interval for the mean hours of TV watched per household:\n\nmean.int.90 <- mean.x + qt( c(0.05, 0.95), length(x) - 1) * SE.x\nmean.int.90\n\n[1] 29.13527 31.14473\n\n\nIn this example, we can state that we are 90% confident that the range [29.14, 31.14] encompasses the true population mean. The function qt finds the two-tailed critical values from Student’s t distribution with length(x) -1 degrees of freedom (or df = 49 in our working example). Note that using Student’s t value is recommended over the normal distribution’s z value when sample size is small. If the sample size is large, Student’s t value and the normal’s z values converge.\nIn the following figure, the 90% confidence range is shaded in the light red color. Recall that the distribution is for the many different sample means that could be drawn from the population and not the distribution of the raw (sampled) data.\n\n\n\n\n\nLikewise, we can compute the 90% confidence interval from the bootstrap \\(SE\\) estimate of the median.\n\nmedian.int.90 <- mean(median.boot$t) + qt( c(0.05, 0.95), length(x) - 1) * SE.boot\nmedian.int.90\n\n[1] 29.34028 31.65448\n\n\nThis example suggests that there is a 90% chance that the range of median values [29.34, 31.65] encompasses the true population median. Note that using standard bootstrap techniques to estimate \\(CI\\) requires that the bootstrap distribution of a statistic follow a normal distribution. If it does not, the computed \\(CI\\) may over- or under-estimate the true confidence interval. A safer (and more robust) bootstrap based measure of \\(CI\\) is the bias corrected bootstrap method, BCa, which can be easily computed using the boot.ci function in the boot library. The parameter conf defines the confidence interval sought.\n\nmedian.int.90.BCa <- boot.ci(median.boot, conf = .90, type=\"bca\")\nmedian.int.90.BCa\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 10000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = median.boot, conf = 0.9, type = \"bca\")\n\nIntervals : \nLevel       BCa          \n90%   (29.4, 31.2 )  \nCalculations and Intervals on Original Scale\n\n\nThe BCa interval values can be extracted from the variable by typing median.int.90.BCa$bca[4:5] at the prompt.\nOn a final note, the margin of error (MoE) can be used interchangeably with the confidence interval. The [American Statistical Association][1] (page 64) defines the MoE as a 95% confidence interval, but this definition is not consistent across the literature. So if given estimates with a measure of confidence defined as a MoE make sure to ask for the provider’s definition of the MoE!\n\n\n6 References\nFreedman D.A., Robert Pisani, Roger Purves. Statistics, 4th edition, 2007.\nMcClave J.T., Dietrich F.H., Statistics, 4th edition, 1988.\n\nSession Info:\n\nR version 4.2.2 (2022-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nattached base packages: stats, graphics, grDevices, utils, datasets, methods and base\nother attached packages: boot(v.1.3-28) and gplots(v.3.1.3)\nloaded via a namespace (and not attached): Rcpp(v.1.0.10), gtools(v.3.9.4), digest(v.0.6.31), bitops(v.1.0-7), Rttf2pt1(v.1.3.12), jsonlite(v.1.8.4), evaluate(v.0.20), KernSmooth(v.2.23-20), rlang(v.1.0.6), cli(v.3.6.0), rstudioapi(v.0.14), extrafontdb(v.1.0), rmarkdown(v.2.20), extrafont(v.0.19), tools(v.4.2.2), pander(v.0.6.5), xfun(v.0.37), yaml(v.2.3.7), fastmap(v.1.1.0), compiler(v.4.2.2), caTools(v.1.18.2), htmltools(v.0.5.4) and knitr(v.1.42)"
  },
  {
    "objectID": "F_test.html",
    "href": "F_test.html",
    "title": "Comparing variances: Fisher’s F-test",
    "section": "",
    "text": "Last modified on 2023-04-06\nThe test of variances requires that the two sampled population be normally distributed and that the samples are randomly selected from their respective populations."
  },
  {
    "objectID": "F_test.html#solution-to-example-1",
    "href": "F_test.html#solution-to-example-1",
    "title": "Comparing variances: Fisher’s F-test",
    "section": "2.1 Solution to example 1",
    "text": "2.1 Solution to example 1\nThe variances for both samples are \\(s_{Ref}^2 = 712.5\\) and \\(s_{Cont}^2 = 336.7\\). Since \\(s_{Ref}^2 > s_{Cont}^2\\), the value \\(s_{Ref}^2\\) will be in the numerator giving us the following test statistic:\n\\[\nF = \\frac{s_{Ref}^2}{s_{Cont}^2} = \\frac{712.5}{336.7} = 2.12\n\\]\nNext, we must determine where the \\(F\\) statistic lies along the \\(F\\)-distribution curve. This requires that we compute the two \\(df\\)’s from the samples to define the shape of the \\(F\\) distribution: \\[\ndf_{Ref} = 8 - 1 = 7\n\\] \\[\ndf_{Cont} = 6 - 1 =5\n\\]\nNow that we have the shape of the \\(F\\)-distribution defined, we can look up the probability of getting an \\(F\\) statistic as extreme as ours, An F-distribution table can be used, or the value can be computed exactly using the function pf():\n\npf(2.12, 7, 5, lower.tail=FALSE)\n\n[1] 0.2126279\n\n\n\n\n\n\n\nThe \\(F\\) values associated with a probability of 0.025 and 0.975 (associated with rejection regions for a two-tailed \\(\\alpha\\) of 0.05) are displayed on the curve in grey dashed vertical lines.\nThe probability of getting an \\(F\\) as large as ours is about 0.21 (or 21%). Since \\(H_a\\) represents both sides of the distribution, we double the probability to give us the chance of getting a test statistic as great or as small as ours, so for a two-tailed test, \\(P=0.42\\). With such a high \\(P\\)-value, we cannot reject the null and therefore can state that for all intents and purposes, the variances between both populations are the same (i.e. the observed variability between both \\(s\\) can be explain by chance alone).\nThe following figure shows the observed \\(P\\) values in both tails.\n\n\n\n\n\nThis can be easily executed in R as a two-tailed test as shown in the following code block:\n\nRef <-  c(560, 530, 570, 490, 510, 550, 550, 530)\nCont <- c(600, 590, 590, 630, 610, 630)\nvar.test(Ref, Cont, alternative=\"two.sided\")\n\n\n    F test to compare two variances\n\ndata:  Ref and Cont\nF = 2.1163, num df = 7, denom df = 5, p-value = 0.4263\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n  0.3088156 11.1853404\nsample estimates:\nratio of variances \n          2.116337 \n\n\nNote that the var.test() computes the \\(F\\) ratio using the first variable name in the list as the numerator. For example, had we reversed the order of variables (i.e. var.test(Cont, Ref, alternative=\"two-sided\")), the returned \\(F\\) value would be the inverse of the original \\(F\\) value, or \\(1/2.12 = 0.47\\). The \\(P\\) value would have stayed the same however."
  },
  {
    "objectID": "F_test.html#solution-to-example-2",
    "href": "F_test.html#solution-to-example-2",
    "title": "Comparing variances: Fisher’s F-test",
    "section": "3.1 Solution to example 2",
    "text": "3.1 Solution to example 2\nWe are asked to test the hypothesis, \\(H_o\\), that the two stock have equal variances and that any observed difference is due to chance (i.e. \\(\\sigma_1^2 = \\sigma_2^2\\)). The alternate hypothesis, \\(H_a\\), states that stock 1 has greater variability than stock 2 (i.e. \\(\\sigma_1^2 > \\sigma_2^2\\)).\nSince we are given summary statistics of the samples and not the full dataset, we cannot use the var.test() function which requires the full dataset as input. Instead, we will compute the \\(F\\) ratio and observed probabilities separately.\nThe \\(F\\) ratio is: \\[\nF = \\frac{(.76)^2}{(.46)^2} = 2.73\n\\]\nThe degrees of freedom are \\((25 - 1) = 24\\) for both samples.\nThe probability of getting a test statistic as extreme as ours can be computed using the pf() function:\n\npf( 2.73, 24, 24, lower.tail = FALSE)\n\n[1] 0.008502252\n\n\nNote that we are using the lower.tail = FALSE option since our alternate hypothesis is that \\(\\sigma_1^2 > \\sigma_2^2\\). This gives us a probability of \\(0.008\\), in other words, if the difference between stock 1 and stock 2 were explained by chance variability alone, there would be lest than a 1% chance of computing a \\(F\\) ratio as extreme as ours. We can safely reject \\(H_o\\) and state that the observed difference is real and that stock 1 has greater daily variability than stock 2."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Basic stats explained (in R)",
    "section": "",
    "text": "Confidence intervals\nComparing means between two batches: z and t tests\nComparing means between two or more batches: ANOVA\nComparing variances: Fisher’s F-test\nComparing frequencies: Chi-Square tests\nRegression analysis (OLS method)\nLogistic regression"
  },
  {
    "objectID": "Logistic.html",
    "href": "Logistic.html",
    "title": "Logistic regression",
    "section": "",
    "text": "Last modified on 2023-04-06\nPackages used in this tutorial:\nAnother package used in this tutorial is gdata, but its function will be called directly from the package (e.g. gdata::mapLevels) in section 2."
  },
  {
    "objectID": "Logistic.html#alternative-pseudo-r2",
    "href": "Logistic.html#alternative-pseudo-r2",
    "title": "Logistic regression",
    "section": "3.1 Alternative pseudo R2",
    "text": "3.1 Alternative pseudo R2\nHere, we’ll make use of the rms package’s lrm function to compute another form of the pseudo R2 called the Nagelkerke R2.\n\nlrm(Coast ~ Income, df)\n\nLogistic Regression Model\n\nlrm(formula = Coast ~ Income, data = df)\n\n                    Model Likelihood    Discrimination    Rank Discrim.    \n                          Ratio Test           Indexes          Indexes    \nObs          16    LR chi2      7.37    R2       0.492    C       0.828    \n no           8    d.f.            1    R2(1,16) 0.329    Dxy     0.656    \n yes          8    Pr(> chi2) 0.0066    R2(1,12) 0.412    gamma   0.656    \nmax |deriv| 0.4                         Brier    0.143    tau-a   0.350    \n\n          Coef     S.E.   Wald Z Pr(>|Z|)\nIntercept -12.2176 5.7646 -2.12  0.0341  \nIncome      0.0005 0.0002  2.10  0.0355  \n\n\nNote how this value of 0.49 differs from that of the Hosmer and Lemeshow R2 whose value is 0.33."
  },
  {
    "objectID": "Logistic.html#model-significance",
    "href": "Logistic.html#model-significance",
    "title": "Logistic regression",
    "section": "4.1 Model significance",
    "text": "4.1 Model significance\nA p-value for the logistic model can be approximated (note that it is difficult to associate an exact p-value with a logistic regression model).\nFirst, pull the the difference in degrees of freedom between the null and full model:\n\nChidf <- M1$df.null - M1$df.residual\n\nThen, compute the p-value using the chi-square statistic. This pseudo p-value is also called the likelihood ratio p-value.\n\nchisq.prob <- 1 - pchisq(modelChi, Chidf)\nchisq.prob\n\n[1] 0.006619229\n\n\nIf the p-value is small then we can reject the null hypothesis that the current model does not improve on the base model. Here, the p-value is 0.01 suggesting that the model is a significant improvement over the base model."
  },
  {
    "objectID": "Logistic.html#parameter-significance",
    "href": "Logistic.html#parameter-significance",
    "title": "Logistic regression",
    "section": "4.2 Parameter significance",
    "text": "4.2 Parameter significance\nIf we want to assess the significance of a parameter as it compares to the base model simply wrap the model object with the summary function.\n\nsummary(M1)\n\n\nCall:\nglm(formula = Coast ~ Income, family = binomial, data = df)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.3578  -0.6948  -0.1863   0.5207   2.2137  \n\nCoefficients:\n               Estimate  Std. Error z value Pr(>|z|)  \n(Intercept) -12.2177062   5.7646456  -2.119   0.0341 *\nIncome        0.0005048   0.0002401   2.102   0.0355 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 22.181  on 15  degrees of freedom\nResidual deviance: 14.807  on 14  degrees of freedom\nAIC: 18.807\n\nNumber of Fisher Scoring iterations: 5\n\n\nThe Income coefficient p-value is 0.036."
  },
  {
    "objectID": "regression.html",
    "href": "regression.html",
    "title": "Regression analysis (OLS method)",
    "section": "",
    "text": "Last modified on 2023-04-06\nPackages used in this tutorial:"
  },
  {
    "objectID": "regression.html#testing-if-the-bivariate-model-is-an-improvement-over-the-mean-model",
    "href": "regression.html#testing-if-the-bivariate-model-is-an-improvement-over-the-mean-model",
    "title": "Regression analysis (OLS method)",
    "section": "2.1 Testing if the bivariate model is an improvement over the mean model",
    "text": "2.1 Testing if the bivariate model is an improvement over the mean model\n\n2.1.1 The coefficient of determination, \\(R^2\\)\nIn this working example, the percent error explained by the bivariate model is,\n\\[\n\\frac{SSE_{mean} - SSE}{SSE_{mean}} \\times 100 = \\frac{SSR}{SSE_{mean}} \\times 100\n\\]\nor (205233706 - 28724435) / 205233706 * 100 = 86.0%. This is a substantial reduction in error.\nThe ratio between \\(SSR\\) and \\(SSE_{mean}\\) is also referred to as the proportional error reduction score (also referred to as the coefficient of determination), or \\(R^2\\), hence:\n\\[\nR^2 = \\frac{SSR}{SSE_{mean}}\n\\]\n\\(R^2\\) is another value computed by the linear model lm() that can be extracted from the output M as follows:\n\nsummary(M)$r.squared\n\n[1] 0.8600404\n\n\nThe \\(R^2\\) value computed by \\(M\\) is the same as that computed manually using the ratio of errors (except that the latter was presented as a percentage and not as a fraction). Another way to describe \\(R^2\\) is to view its value as the fraction of the variance in \\(Y\\) explained by \\(X\\). A \\(R^2\\) value of \\(0\\) implies complete lack of fit of the model to the data whereas a value of \\(1\\) implies perfect fit. In our working example the \\(R^2\\) value of 0.86 implies that the model explains 86% of the variance in \\(Y\\).\nR also outputs \\(adjusted\\; R^2\\), a better measure of overall model fit. It ‘penalizes’ \\(R^2\\) for the number of predictors in the model vis-a-vis the number of observations. As the ratio of predictors to number of observation increases, \\(R^2\\) can be artificially inflated thus providing us with a false sense of model fit quality. If the number of predictors to number of observations ratio is small, \\(adjusted\\; R^2\\) will be smaller than \\(R^2\\). \\(adjusted\\; R^2\\) can be extracted from the model output as follows:\n\nsummary(M)$adj.r.squared\n\nThe \\(adjusted\\; R^2\\) of 0.85 is very close to our \\(R^2\\) value of 0.86. So our predictive power hasn’t changed given our sample size and one predictor variable.\n\n\n2.1.2 The F-ratio test\n\\(R^2\\) is one way to evaluate the strength of the linear model. Another is to determine if the reduction in error between the bivariate model and mean model is significant. We’ve already noted a decrease in overall residual errors when augmenting our mean model with a predictor variable \\(X\\) (i.e. the fraction of residents having attained at least a bachelor’s degree). Now we need to determine if this difference is significant.\nIt helps to conceptualize residual errors as spreads or variances. The following two plots show the errors explained by the model (left) and the errors not explained by the model for each of the 16 data points. Note that we can derive \\(SSR\\) and \\(SSE\\) from the left and right graphs respectively by squaring then summing the horizontal error bars.\n\n\n\n\n\nWe want to assess whether the total amount of unexplained error (right) is significantly less than the total amount of explained error (left). We have 16 records (i.e. data for sixteen counties) therefore we have 16 measures of error. Since the model assumes that the errors are the same across the entire range of \\(Y\\) values, we will take the average of those errors; hence we take the average of \\(SSR\\) and the average of \\(SSE\\). However, because these data are from a subset of possible \\(Y\\) values and not all possible \\(Y\\) values we will need to divide \\(SSR\\) and \\(SSE\\) by their respective \\(degress\\; of\\; freedom\\) and not by the total number of records, \\(n\\). These “averages” are called mean squares (\\(MS\\)) and are computed as follows:\n\\[\nMS_{model} = \\frac{SSR}{df_{model}} =  \\frac{SSR}{parameters\\; in\\; the\\; bivariate\\; model - parameters\\; in\\; the\\; mean\\; model}\n\\] \\[\nMS_{residual} = \\frac{SSE}{df_{residual}} = \\frac{SSE}{n - number\\; of\\; parameters}\n\\]\nwhere the \\(parameters\\) are the number of coefficients in the mean model (where there is just one parameter: \\(\\beta_0 = \\hat Y\\)) and the number of coefficient in the bivariate model (where there are two parameters: \\(\\beta_0\\) and \\(\\beta_1\\)). In our working example, \\(df_{model}\\) = 1 and \\(df_{residual}\\) = 14.\nIt’s important to remember that these measures of error are measures of spread and not of a central value. Since these are measures of spread we use the \\(F\\)-test (and not the \\(t\\)-test) to determine if the difference between \\(MS_{model}\\) and \\(MS_{residual}\\) is significant (recall that the \\(F\\) test is used to determine if two measures of spread between samples are significantly different). The \\(F\\) ratio is computed as follows:\n\\[\nF = \\frac{MS_{model}}{MS_{residual}}\n\\]\nA simple way to implement an \\(F\\) test with on our bivariate regression model is to call the anova() function as follows:\n\nanova(M)\n\nwhere the variable M is the output from our regression model created earlier. The output of anova() is summarized in the following table:\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(>F)\n\n\n\n\nx\n1\n176509271\n176509271\n86.02884\n0.0000002\n\n\nResiduals\n14\n28724435\n2051745\nNA\nNA\n\n\n\n\nThe first row in the table (labeled as x) represents the error terms \\(SSR\\) and \\(MS_{model}\\) for the basic model (the mean model in our case) while the second row labeled Residuals represents error terms \\(SSE\\) and \\(MS_{residual}\\) for the current model. The column labeled Df displays the degrees of freedom, the column labeled Sum Sq displays the sum of squares \\(SSR\\) and \\(SSE\\), the column labeled Mean Sq displays the mean squares \\(MS\\) and the column labeled F value displays the \\(F\\) ratio \\(MS_{model}/MS_{residual}\\).\nThe larger the \\(F\\) ratio, the greater the difference between the bivariate model and mean model. The question then becomes ‘how significant is the observed ratio?’. To answer this question, we must setup a hypothesis test. We setup the test as follows:\n\\(H_o\\): The addition of the term \\(\\beta_1\\) (and hence the predictor \\(X\\)) does not improve the prediction of \\(Y\\) over the simple mean model.\n\\(H_a\\): The addition of the term \\(\\beta_1\\) (and hence the predictor \\(X\\)) helps improve the prediction of \\(Y\\) over the simple mean model.\nThe next step is to determine how likely we are (as a measure of probability) to have a \\(F\\) ratio as large as the one observed under the assumption \\(H_o\\) that the bivariate model does not improve on our prediction of \\(Y\\). The last column in the anova output gives us this probability (as a fraction): \\(P\\) = 0. In other words, there is a 0% chance that we could have computed an \\(F\\) ratio as extreme as ours had the bivariate model not improved over the mean model.\nThe following graph shows the frequency distribution of \\(F\\) values we would expect if indeed \\(H_o\\) was true. Most of the values under \\(H_o\\) fall between 0 and 5. The red line to the far right of the curve is where our observed \\(F\\) value lies along this continuum–this is not a value one would expect to get if \\(H_o\\) were true. It’s clear from this output that our bivariate model is a significant improvement over our mean model.\n\n\n\n\n\nThe F-ratio is not only used to compare the bivariate model to the mean, in fact, it’s more commonly used to compare a two-predictor variable model with a one-predictor variable model or a three-predictor model with a two-predictor model and so on. In each case, one model is compared with a similar model minus one predictor variable.\nWhen two different models are compared, the function anova() requires two parameters: the two regression model outputs (we’ll call M1 and M2).\n\nanova(M1, M2)\n\nAn example will be provided in a later section when we tackle multivariate regression."
  },
  {
    "objectID": "regression.html#testing-if-the-estimated-slope-is-significantly-different-from-0",
    "href": "regression.html#testing-if-the-estimated-slope-is-significantly-different-from-0",
    "title": "Regression analysis (OLS method)",
    "section": "2.2 Testing if the estimated slope is significantly different from 0",
    "text": "2.2 Testing if the estimated slope is significantly different from 0\nWe’ve just assessed that our bivariate model is a significant improvement over the much simpler mean model. We can also test if each regression coefficient \\(\\beta\\) is significantly different from 0. If we are dealing with a model that has just one predictor \\(X\\), then the \\(F\\) test just described will also tell us if the regression coefficient \\(\\beta_1\\) is significant. However, if more than one predictor variable is present in the regression model, then you should perform an \\(F\\)-test to test overall model improvement, then test each regression term independently for significance.\nWhen assessing if a regression coefficient is significantly different from zero, we are setting up yet another hypothesis test where:\n\\(H_o\\): \\(\\beta_1\\) = 0 (i.e. the predictor variable does not contribute significantly to the overall improvement of the predictive ability of the model).\n\\(H_a\\): \\(\\beta_1\\) < 0 or \\(\\beta_1\\) > 0 for one-tailed test, or \\(\\beta_1 \\neq\\) 0 for a two-tailed test (i.e. the predictor variable does contribute significantly to the overall improvement of the predictive ability of the model).\nOur hypothesis in this working example is that the level of education attainment (\\(X\\)) can predict per-capita income (\\(Y\\)), at least at the county level (which is the level at which our data is aggregated). So what we set out to test is the null hypothesis, \\(H_o\\), that \\(X\\) and \\(Y\\) are not linearly related. If \\(H_o\\) is true, then we would expect the regression coefficient \\(\\beta_1\\) to be very close to zero. So this begs the question ‘how close should \\(\\beta_1\\) be to 0 for us not to reject \\(H_o\\)?’\nTo answer this question, we need to perform a \\(t\\)-test where we compare our calculated regression coefficient \\(\\hat \\beta_1\\) to what we would expect to get if \\(X\\) did not contribute significantly to the model’s predictive capabilities. So the statistic \\(t\\) is computed as follows:\n\\[\nt = \\frac{\\hat \\beta_1 - \\beta_{1_o}}{s_{\\hat \\beta_1}}\n\\]\nSince \\(\\beta_1\\) is zero under \\(H_o\\) , we can rewrite the above equation as follows: \\[\nt = \\frac{\\hat \\beta_1 - 0}{s_{\\hat \\beta_1}} = \\frac{\\hat \\beta_1 }{s_{\\hat \\beta_1}}\n\\]\nwhere the estimate \\(s_{\\hat \\beta_1}\\) is the standard error of \\(\\beta\\) which can be computed from:\n\\[\ns_{\\hat \\beta_1} = \\frac{\\sqrt{MS_{residual}}}{\\sqrt{SSX}}\n\\]\nwhere \\(SSX\\) is the sum of squares of the variable \\(X\\) (i.e. \\(\\sum (X - \\bar X)^2\\)) which was computed in an earlier table.\nIn our working example, \\(s_{\\hat \\beta_1}\\) is 1432.3914787 / 0.2674766 or \\(s_{\\hat \\beta_1}\\) = 5355.2. The test statistic \\(t\\) is therefore 49670.43 / 5355.2 or \\(t\\) = 9.28.\nNext we need to determine if the computed value of \\(t\\) is significantly different from the values of \\(t\\) expected under \\(H_o\\). The following figure plots the frequency distribution of \\(\\beta_{1_o}\\) (i.e. the kind of \\(\\beta_1\\) values we could expect to get under the assumption that the predictor \\(X\\) does not contribute to the model) along with the red regions showing where our observed \\(\\hat \\beta_1\\) must lie for us to safely reject \\(H_o\\) at the 5% confidence level (note that because we are performing a two-sided test, i.e. that \\(\\hat \\beta_1\\) is different from 0, the red regions each represent 2.5%; when combined both tails give us a 5% rejection region).\n\n\n\n\n\nIt’s clear that seeing where our observed \\(\\hat \\beta_1\\) lies along the distribution allows us to be fairly confident that our \\(\\hat \\beta_1\\) is far from being a typical value under \\(H_o\\), in fact we can be quite confident in rejecting the null hypothesis, \\(H_o\\). This implies that \\(X\\) (education attainment) does contribute information towards the prediction of \\(Y\\) (per-capita income) when modeled as a linear relationship.\nFortunately, we do not need to burden ourselves with all these calculations. The aforementioned standard error, \\(t\\) value and \\(P\\) value are computed as part of the lm analysis. You can list these and many other output parameters using the summary() function as follows:\n\nsummary(M)[[4]]\n\nThe [[4]] option displays the pertinent subset of the summary output that we want. The content of this output is displayed in the following table:\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(>|t|)\n\n\n\n\n(Intercept)\n12404.50\n1350.332\n9.186264\n0.0000003\n\n\nx\n49670.43\n5355.202\n9.275173\n0.0000002\n\n\n\n\nThe second line of the output displays the value of \\(\\hat \\beta_1\\), its standard error, its \\(t\\)-value and the probability of observing a \\(\\beta_1\\) value as extreme as ours under \\(H_o\\).\n\n2.2.1 Extracting \\(\\hat \\beta_1\\) confidence interval\n\\(\\hat \\beta_1\\)’s standard error \\(s_{\\hat \\beta_1}\\) can also be used to derive a confidence interval for our estimate. The standard error of 5355 tells us that we can be ~68% confident that our true \\(\\beta_1\\) value lies between \\(\\hat \\beta_1\\) - 5355 and \\(\\hat \\beta_1\\) + 5355. We can use the standard error to construct different confidence intervals depending on our desired \\(\\alpha\\) level. The following table shows upper and lower limits for a few common alpha levels:\n\n\n\n\n\n\nConfidence level \\(\\alpha\\)\nLower limit\nUpper limit\n\n\n\n\n68%\n44149\n55192\n\n\n95%\n38185\n61156\n\n\n99%\n33729\n65612\n\n\n\nThe following figure shows the range of the distribution curve covered by the 68% and 95% confidence intervals for our working example.\n\n\n\n\n\nThe confidence intervals for \\(\\hat \\beta_1\\) can easily be extracted in R using the confint() function. for example, to get the confidence interval for an \\(\\alpha\\) of 95%, call the following function:\n\nconfint(M, parm = 2, level = 0.95)\n\n     2.5 %  97.5 %\nx 38184.66 61156.2\n\n\nThe first parameter, M, is the linear regression model output computed earlier. The second parameter parm = 2 tells R which parameter to compute the confidence interval for (a value of 1 means that the confidence interval for the intercept is desired), and the third parameter in the function, level = 0.95 tells R which \\(\\alpha\\) value you wish to compute a confidence interval for."
  },
  {
    "objectID": "regression.html#checking-the-residuals",
    "href": "regression.html#checking-the-residuals",
    "title": "Regression analysis (OLS method)",
    "section": "2.3 Checking the residuals",
    "text": "2.3 Checking the residuals\nWe have already used the residuals \\(\\varepsilon\\) to give us and assessment of model fit and the contribution of \\(X\\) to the prediction of \\(Y\\). But the distribution of the residuals can also offer us insight into whether or not all of the following key assumptions are met:\n\nThe mean of the residuals, \\(\\bar \\varepsilon = 0\\), is close to 0.\nThe spread of the residuals is the same for all values of \\(X\\)–i.e. they should be homoscedastic.\nThe probability distribution of the errors follows a normal (Gaussian) distribution.\nThe residuals are independent of one another–i.e. they are not autocorrelated.\n\nWe will explore each assumption in the following sections.\n\n2.3.1 Assumption 1: \\(\\bar \\varepsilon = 0\\)\nThe lm regression model outputs not only parameters as seen in earlier sections, but residuals as well. For each \\(Y\\) value, a residual (or error) accounting for the difference between the model \\(\\hat Y\\) and actual \\(Y\\) value is computed. We can plot the residuals as a function of the predictor variable \\(X\\). We will also draw the line at \\(\\varepsilon\\) = 0 to see how \\(\\varepsilon\\) fluctuates around 0.\n\nplot(M$residuals ~ x, pch = 16, ylab= \"Residuals\")\nabline(h = 0, lty = 3)\n\n\n\n\nWe can compute the mean of the residuals, \\(\\bar \\varepsilon\\), as follows:\n\nmean(M$residuals)\n\n[1] -0.00000000000002131628\n\n\nwhich gives us a value very close to zero as expected.\n\n\n2.3.2 Assumption 2: homoscedasticity\nIt helps to demonstrate via figures what does and does not constitute a homoscedastic residual:\n\n\n\n\n\nThe figure on the left shows no obvious trend in the variability of the residuals as a function of \\(X\\). The middle figure shows an increase in variability of the residuals with increasing \\(X\\). The figure on the right shows a decrease in variability in the residuals with increasing \\(X\\). When the variances of \\(\\varepsilon\\) are not uniform across all ranges of \\(X\\), the residuals are said to be heteroscedastic.\nOur plot of the residuals from the bivariate model may show some signs of heteroscedasticity, particularly on the right hand side of the graph. When dealing with relatively small samples, a visual assessment of the distribution of the residuals may not be enough to assess whether or not the assumption of homoscedasticity is met. We therefore turn to the Breusch-Pagan test. A Breusch-Pagan function, ncvTest(), is available in the library car which will need to be loaded into our R session before performing the test. The package car is usually installed as part of the normal R installation.\n\nlibrary(car)\nncvTest(M)\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 0.2935096, Df = 1, p = 0.58798\n\n\nThe Breusch-Pagan tests the (null) hypothesis that the variances are constant across the full range of \\(X\\). The \\(P\\) value tells the likelihood that our distribution of residual variances are consistent with homoscedasticity. A non-significant \\(P\\) value (one usually greater than 0.05) should indicate that our residuals are homoscedastic. Given our high \\(P\\) value of 0.59, we can be fairly confident that we have satisfied our second assumption.\n\n\n2.3.3 Assumption 3: Normal (Gaussian) distribution of residuals\nAn important assumption is that the distribution of the residual values follow a normal (Gaussian) distribution. Note that this assumption is only needed when computing confidence intervals or regression coefficient \\(P\\)-values. If your interest is solely in finding the best linear unbiased estimator (BLUE), then only the three other assumptions need to be met.\nThe assumption of normality of residuals is often confused with the belief that \\(X\\) (the predictor variables) must follow a normal distribution. This is incorrect! The least-squares regression model makes no assumptions about the distribution of \\(X\\). However, if the residuals do not follow a normal distribution, then one method of resolving this problem is to transform the values of \\(X\\) (which may be the source of confusion).\nIt helps to first plot the histogram of our residuals then to fit a kernel density function to help see the overall trend in distribution.\n\nhist(M$residuals, col=\"bisque\", freq=FALSE, main=NA)\nlines(density(M$residuals), col=\"red\")\n\n\n\n\nA good visual test of normality is the use of a Q-Q plot.\n\nplot(M, which = 2)\n\n\n\n\nWhat we are hoping to see is an alignment of the residual values (hollow points on the plot) along the sloped dashed line. Any deviation from the sloped line may indicate lack of normality in our residuals. You will seldom encounter residuals that fit the line exactly. What you are assessing is whether or not the difference is significant. In our working example, there seems to be disagreement between the distribution of our residuals and the normal line on the plot.\nA visual assessment of normality using the Q-Q plot is usually the preferred approach, but you can also use the Shapiro-Wilk test to assess the significance of deviation from normality. But use this test with caution, large sample sizes tend to always indicate deviation from normality, regardless how close they follow the Gaussian curve. R has a function called shapiro.test() which we can call as follows:\n\nshapiro.test(M$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  M$residuals\nW = 0.92243, p-value = 0.1846\n\n\nThe output we are interested in is the \\(P\\)-value. The Shapiro-Wilk Test tests the null hypothesis that the residuals come from a normally distributed population. A large \\(P\\)-value indicates that there is a good chance that the null is true (i.e. that the residuals are close to being normally distributed). If the \\(P\\) value is small, then there is a good chance that the null is false. Our \\(P\\) value is 0.18 which would indicate that we cannot reject the null. Despite the Shapiro-Welk test telling us that there is a good chance that our \\(\\varepsilon\\) values come from a normally distributed population, our visual assessment of the Q-Q plot leads us to question this assessment.\nAgain, remember that the assumption of normality of residuals really only matters when you are seeking a confidence interval for \\(\\hat \\beta_1\\) or \\(P\\)-values.\n\n\n2.3.4 Assumption 4: Independence of residuals\nOur final assumption pertains to the independence of the residuals. In other words, we want to make sure that a residual value is not auto-correlated with a neighboring residual value. The word neighbor can mean different things. It may refer to the order of the residuals in which case we are concerned with residuals being more similar within a narrow range of \\(X\\), or it may refer to a spatial neighborhood in which case we are concerned with residuals of similar values being spatially clustered. To deal with the former, we can use the Durbin-Watson test available in the R package car.\n\nlibrary(car) # Load this package if not already loaded\ndurbinWatsonTest(M)\n\n lag Autocorrelation D-W Statistic p-value\n   1      0.01856517      1.706932   0.588\n Alternative hypothesis: rho != 0\n\n\nIdeally, the D-W statistic returned by the test should fall within the range of 1 to 3. The \\(P\\)-value is the probability that our residual distribution is consistent with what we would expect if there was no auto-correlation. Our test statistic of 1.71 and \\(P\\) value of 0.62 suggests that the assumption of independence is met with our model. You might note that the \\(P\\)-value changes every time the tests is re-run. This is because the Durbin Watson test, as implemented in R, uses a Monte-Carlo approach to compute \\(P\\). If you want to nail \\(P\\) down to a greater precision, you can add the reps = parameter to the function (by default, the test runs 1000 bootstrap replications). For example, you could rerun the test using 10,000 bootstrap replications (durbinWatsonTest(M, reps = 10000))."
  },
  {
    "objectID": "regression.html#influential-observations",
    "href": "regression.html#influential-observations",
    "title": "Regression analysis (OLS method)",
    "section": "2.4 Influential Observations",
    "text": "2.4 Influential Observations\nWe want to avoid a situation where a single, or very small subset of points, have a disproportionately large influence on the model results. It is usually best to remove such influential points from the regression analysis. This is not to say that the influential points should be ignored from the overall analysis, but it may suggest that such points may behave differently then the bulk of the data and therefore may require a different model.\nSeveral tests are available to determine if one or several points are influential. Two of which are covered here: Cook’s distance and hat values.\nCook’s distance can be computed from the model using the cooks.distance() function. But it’s also available as one of plot.lm’s diagnostic outputs.\n\nplot( M, which = 4)\n\n\n\n\nWe could have called the plot.lm function, but because R recognizes that the object M is the output of an lm regression, it automatically passes the call to plot.lm. The option which = 4 tells R which of the 6 diagnostic plots to display (to see a list of all diagnostic plots offered, type ?plot.lm). Usually, any point with a Cook’s distance value greater than 1 is considered overly influential. In our working example, none of the points are even close to 1 implying that none of our observations wield undue influence.\nThe other test that can be used to assess if a point has strong leverage is the hat values test. The technique involves calculating an average leverage value for all data points; this is simply the ratio between the number of regression coefficients in our model (this includes the intercept) and the number of observations. Once the average leverage is computed, we use a cutoff of either twice this average or three times this average (these are two popular cutoff values). We then look for hat values greater then these cutoffs. In the following chunk of code, we first compute the mean leverage values (whose value is assigned to the cut object), we then compute the leverage values for each point using the hatvalues() function and plot the resulting leverage values along with the two cutoff values (in dashed red lines).\n\ncut   <- c(2, 3) * length(coefficients(M)) / length(resid(M))\nM.hat <- hatvalues(M)\nplot(M.hat)\nabline(h = cut, col = \"red\", lty = 2)\ntext( which(M.hat > min(cut) ) ,  M.hat[M.hat > min(cut)] , \n      row.names(dat)[M.hat > min(cut)], pos=4, col=\"red\")\n\n\n\n\nThe last line of code (the one featuring the text function) labels only the points having a value greater than the smaller of the two cutoff values.\nThe influential point of interest is associated with the third record in our dat dataset, Cumberland county. Let’s see where the point lies relative to the regression slope\n\nplot(Income ~ Education, dat)\nabline(M, col=\"blue\")\npoints(dat[M.hat > min(cut),]$Education, \n       dat[M.hat > min(cut),]$Income, col=\"red\", pch=16) \ntext( dat[M.hat > min(cut),]$Education, \n       dat[M.hat > min(cut),]$Income , \n       labels = row.names(dat)[M.hat > min(cut)], \n      col=\"red\", pos=2) # Add label to the left of point\n\n\n\n\nThe point lies right on the line and happens to be at the far end of the distribution. The concern is that this point might have undue leverage potential. It helps to think of the regression line as a long straight bar hinged somewhere near the center. It requires less force to move the bar about the imaginary hinge point when applied to the ends of the bar then near the center of the bar. The hat values test is suggesting that observation number 3 may have unusual leverage potential. We can assess observation number 3’s influence by running a new regression analysis without observation number 3.\n\n# Create a new dataframe that omits observation number 3\ndat.nolev <- dat[ - which(M.hat > min(cut)),]\n\n# Run a new regression analysis\nM.nolev <- lm( Income ~ Education, dat.nolev)\n\nWe can view a full summary of this analysis using the summary() function.\n\nsummary(M.nolev)\n\n\nCall:\nlm(formula = Income ~ Education, data = dat.nolev)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2730.0  -518.1   306.4   819.1  2009.8 \n\nCoefficients:\n            Estimate Std. Error t value   Pr(>|t|)    \n(Intercept)    12408       1670   7.431 0.00000497 ***\nEducation      49654       6984   7.109 0.00000794 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1486 on 13 degrees of freedom\nMultiple R-squared:  0.7954,    Adjusted R-squared:  0.7797 \nF-statistic: 50.54 on 1 and 13 DF,  p-value: 0.000007942\n\n\nYou’ll note that the \\(adjusted\\; R^2\\) value drops from 0.85 to 0.78.\nIf we compare the estimates (and their standard errors) between both model (see the summary tables below), we’ll note that the estimates are nearly identical, however, the standard errors for \\(\\hat \\beta_1\\) increase by almost 30%.\nThe original model M:\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(>|t|)\n\n\n\n\n(Intercept)\n12404.50\n1350.33\n9.19\n0\n\n\nx\n49670.43\n5355.20\n9.28\n0\n\n\n\n\nThe new model M.nolev:\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(>|t|)\n\n\n\n\n(Intercept)\n12407.93\n1669.77\n7.43\n0\n\n\nEducation\n49654.45\n6984.52\n7.11\n0\n\n\n\n\nGiven that our estimates are nearly identical, but that the overall confidence in the model decreases with the new model, there seems to be no reason why we would omit the 3rd observation based on this analysis.\nThis little exercise demonstrates the need to use as many different tools as possible to evaluate whether an assumption is satisfied or not."
  },
  {
    "objectID": "regression.html#what-to-do-if-some-of-the-assumptions-are-not-met",
    "href": "regression.html#what-to-do-if-some-of-the-assumptions-are-not-met",
    "title": "Regression analysis (OLS method)",
    "section": "2.5 What to do if some of the assumptions are not met",
    "text": "2.5 What to do if some of the assumptions are not met\n\n2.5.1 Data transformation\nData transformation (usually via some non-linear re-expression) may be required when one or more of the following apply: * The residuals are skewed or are heteroscedastic. * When theory suggests. * To force a linear relationship between variables\nDo not transform your data if the sole purpose is to “correct” for outliers.\nWe observed that our residuals did not follow a normal (Gaussian) distribution. Satisfying this assumption is important if we are to use this model to derive confidence intervals around our \\(\\beta\\) estimates or if we are to use this model to compute \\(P\\) values. We can see if transforming our data will help. This means re-expressing the \\(X\\) and/or the \\(Y\\) values (i.e. converting each value in \\(X\\) and/or \\(Y\\) by some expression such as log(x) or log(y))\nIt helps to look at the distribution of the variables (this is something that should normally be done prior to performing a regression analysis). We’ll use the hist() plotting function.\n\nhist(dat$Income, breaks = with(dat, seq(min(Income), max(Income), length.out=5)) )\nhist(dat$Education, breaks = with(dat, seq(min(Education), max(Education), length.out=5)) )\n\n\n\n\n\n\nTwo common transformations are the natural log (implemented as log in R) and the square. These are special cases of what is called a Box-Cox family of transformations.\nData transformation is an iterative process which relies heavily on Exploratory Data Analysis (EDA) skills. It’s important to keep in mind the goal of the transformation. In our example, its purpose is to help improve the symmetry of the residuals. The following figures show different Q-Q plots of the regression residuals for different transformations of \\(X\\) and \\(Y\\).\nSeveral data transformation scenarios were explored using square root and logarithmic transformation of the \\(X\\)’s and/or \\(Y\\)’s. Of course, a thorough analysis would consist of a wide range of Box-Cox transformations. The scatter plots (along with the regression lines) are displayed on the left hand side for different transformations (the axes labels indicate which, if any, transformation was applied). The accompanying Q-Q plots are displayed to the right of the scatter plots.\n\n\n\n\n\nA visual assessment of the Q-Q plots indicates some mild improvements near the middle of the residual distribution (note how the mid-points line up nicely with the dashed line), particularly in the second and fourth transformation (i.e. for log(Income) ~ Education and sqrt(Income) ~ Education).\nNote that transforming the data can come at a cost, particularly when interpretation of the results is required. For example, what does a log transformation of Income imply? When dealing with concentrations, it can make theoretical sense to take the log of a value–a good example being the concentration of hydrogen ions in a solution which is usually expressed as a log (pH). It’s always good practice to take a step back from the data transformation workflow and assess where your regression analysis is heading.\n\n\n2.5.2 Bootstrapping\nIf the assumption of normality of residual is not met, one can overcome this problem by using bootstrapping techniques to come up with regression parameters confidence intervals and \\(P\\)-values. The bootstrap technique involves rerunning an analysis, such as the regression analysis in our case, many times, while randomly resampling from our data each time. In concept, we are acting as though our original sample is the actual population and we are sampling, at random (with replacement), from this pseudo population.\nOne way to implement a bootstrap is to use the boot package’s function boot(). But before we do, we will need to create our own regression function that will be passed to boot(). This custom function will not only compute a regression model, but it will also return the regression coefficients for each simulation. The following block of code defines our new custom function lm.sim:\n\nlm.sim <- function(formula, data, i)\n{\n  d.sim <- data [i,]  \n  M.sim <- lm(formula, data=d.sim)\n  return(M.sim$coef)\n}\n\nWe can now use the boot() function to run the simulation. Note the call to our custom function lm.sim and the number of bootstrap replicates (i.e. R = 999). Also, don’t forget to load the boot library into the current R session (if not already loaded).\n\nlibrary(boot)\nM.boot <- boot(statistic = lm.sim, formula = Income ~ Education, data=dat, R = 999)\n\nThe results of our bootstrap simulation are now stored in the object M.boot. In essence, a new regression line is created for each simulation. In our working example, we created 999 different regression lines. The following plot shows the first 100 regression lines in light grey. Note how they fluctuate about the original regression line (shown in red). The distribution of these slopes is what is used to compute the confidence interval.\n\n\n\n\n\nWe can now use the function boot.ci (also in the boot package) to extract confidence intervals for our \\(\\beta\\) parameters for a given \\(\\alpha\\) confidence value. For example, to get the confidence intervals for the parameters \\(\\hat \\beta_0\\) at a 95% confidence interval, you can type the following command:\n\nboot.ci(M.boot, conf = 0.95, type = \"bca\", index=1)\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 999 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = M.boot, conf = 0.95, type = \"bca\", index = 1)\n\nIntervals : \nLevel       BCa          \n95%   ( 9933, 14027 )  \nCalculations and Intervals on Original Scale\n\n\nNote that your values may differ since the results stem from Monte Carlo simulation. Index 1, the first parameter in our lm.sim output, is the coefficient for \\(\\hat \\beta_0\\). boot.ci will generate many difference confidence intervals. In this example, we are choosing bca (adjusted bootstrap percentile).\nTo get the confidence interval for \\(\\hat \\beta_1\\) at the 95% \\(\\alpha\\) level just have the function point to index 2:\n\nboot.ci(M.boot, conf = 0.95, type=\"bca\", index=2)\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 999 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = M.boot, conf = 0.95, type = \"bca\", index = 2)\n\nIntervals : \nLevel       BCa          \n95%   (43317, 58859 )  \nCalculations and Intervals on Original Scale\n\n\n\n\n\nThe following table summarizes the 95% confidence interval for \\(\\hat \\beta_1\\) from our simulation. The original confidence interval is displayed for comparison.\n\n\n\nModel\n\\(\\hat \\beta_1\\) interval\n\n\n\n\nOriginal\n[44149, 55192]\n\n\nBootstrap\n[43317, 58859]"
  },
  {
    "objectID": "regression.html#reviewing-the-multivariate-regression-model-summary",
    "href": "regression.html#reviewing-the-multivariate-regression-model-summary",
    "title": "Regression analysis (OLS method)",
    "section": "3.1 Reviewing the multivariate regression model summary",
    "text": "3.1 Reviewing the multivariate regression model summary\nWe can, of course, generate a full summary of the regression results as follows:\n\nsummary(M2)\n\n\nCall:\nlm(formula = Income ~ Education + Professional, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1704.2  -360.6  -202.5   412.8  2006.7 \n\nCoefficients:\n             Estimate Std. Error t value    Pr(>|t|)    \n(Intercept)     10907       1143   9.544 0.000000308 ***\nEducation       29684       7452   3.983     0.00156 ** \nProfessional    84473      26184   3.226     0.00663 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1108 on 13 degrees of freedom\nMultiple R-squared:  0.9223,    Adjusted R-squared:  0.9103 \nF-statistic: 77.13 on 2 and 13 DF,  p-value: 0.00000006148\n\n\nThe interpretation of the model statistics is the same with a multivariate model as it is with a bivariate model. The one difference is that the extra regression coefficient \\(\\hat \\beta_2\\) (associated with the Professional variable) is added to the list of regression parameters. In our example, \\(\\hat \\beta_2\\) is significantly different from \\(0\\) (\\(P\\) = 0.0066).\nOne noticeable difference in the summary output is the presence of a Multiple R-squared statistic in lieu of the bivariate simple R-squared. Its interpretation is, however, the same and we can note an increase in the models’ overall \\(R^2\\) value. The \\(F\\)-statistic’s \\(P\\) value of \\(0\\) tells us that the amount of residual error from this multivariate model is significantly less than that of the simpler model, the mean \\(\\bar Y\\). So far, things look encouraging.\nThe equation of the modeled plane is thus:\n\\(\\widehat{Income}\\) = 10907 + 29684 (\\(Education\\)) + 84473 (\\(Professional\\))"
  },
  {
    "objectID": "regression.html#comparing-the-two-variable-model-with-the-one-variable-model",
    "href": "regression.html#comparing-the-two-variable-model-with-the-one-variable-model",
    "title": "Regression analysis (OLS method)",
    "section": "3.2 Comparing the two-variable model with the one-variable model",
    "text": "3.2 Comparing the two-variable model with the one-variable model\nWe can compare the two-variable model, M2, with the one-variable model (the bivariate model), M1. Using the anova() function. This test assesses whether or not adding the second variable, x2, significantly improves our ability to predict \\(Y\\). It’s important to remember that a good model is a parsimonious one. If an augmented regression model does not significantly improve our overall predictive ability, then we should always revert back to the simpler model.\n\nanova(M1, M2)\n\nAnalysis of Variance Table\n\nModel 1: Income ~ Education\nModel 2: Income ~ Education + Professional\n  Res.Df      RSS Df Sum of Sq      F   Pr(>F)   \n1     14 28724435                                \n2     13 15952383  1  12772052 10.408 0.006625 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe \\(P\\) value is very small indicating that the reduction in residual error in model M2 over model M1 is significant. Model M2 looks promising so far."
  },
  {
    "objectID": "regression.html#looking-for-multi-collinearity",
    "href": "regression.html#looking-for-multi-collinearity",
    "title": "Regression analysis (OLS method)",
    "section": "3.3 Looking for multi-collinearity",
    "text": "3.3 Looking for multi-collinearity\nWhen adding explanatory variables to a model, one must be very careful not to add variables that explain the same ‘thing’. In other words, we don’t want (nor need) to have two or more variables in the model explain the same variance. One popular test for Multi-collinearity is the Variance Inflation Factor (VIF) test. The VIF is computed for all regression parameters. For example, VIF for \\(X_2\\) is computed by first regressing \\(X_1\\) against all other predictors (in our example, we would regress \\(X_1\\) against \\(X_2\\)) then taking the resulting \\(R^2\\) and plugging it in the following VIF equation: \\[\nVIF = \\frac{1}{1 - R^2_1}\n\\]\nwhere the subscript \\(1\\) in \\(R^2_1\\) indicates that the \\(R^2\\) is for the model x1 ~ x2. The VIF for \\(X_2\\) is computed in the same way.\nWhat we are avoiding are large VIF values (a large VIF value may indicate high multicollinearity). What constitutes a large VIF value is open for debate. Typical values range from 3 to 10, so VIF’s should be interpreted with caution.\nWe can use R’s vif() function to compute VIF for the variables Education and Professional\n\nvif(M2)\n\n   Education Professional \n    3.237621     3.237621 \n\n\nThe moderately high VIF should be a warning. It’s always good practice to generate a scatter plot matrix to view any potential relationship between all variables involved. We can use R’s pairs() function to display the scatter plots:\n\npairs(dat, panel = panel.smooth)\n\n\n\n\nThe function pairs() generates a scatter plot matrix of all columns in the dataframe. Three columns of data are present in our dataframe dat, therefore we have a 3x3 scatter plot matrix. We also add a LOWESS smoother (shown as a red polyline in each graph) by calling the option panel = panel.smooth. To interpret the output, just match each plot to its corresponding variable along the x-axis and y-axis. For example, the scatter plot in the upper right-hand corner is that for the variables Professional and Income. We are interested in seeing how the predictor variables Education and Professional relate to one another. The scatter plot between both predictor variables seem to indicate a significant relationship between the two predictors–this does not bode well for our two-predictor model. The fact that they seem to be correlated suggests that they may be explaining the same variance in \\(Y\\). This means that despite having promising model summary statistics, we cannot trust this model and thus revert back to our bivariate model M1."
  },
  {
    "objectID": "regression.html#two-category-variable",
    "href": "regression.html#two-category-variable",
    "title": "Regression analysis (OLS method)",
    "section": "4.1 Two category variable",
    "text": "4.1 Two category variable\nSo far, we have worked with continuous variables. We can include categorical variables in our model as well. For example, let’s hypothesize that coastal counties might be conducive to higher incomes. We’ll create a new variable, x3, that will house one of two values, yes or no, indicating whether or not the county is on the coast or not.\n\nx3 <- c(\"no\", \"no\", \"yes\", \"no\", \"yes\", \"no\", \"yes\", \"yes\",\n        \"no\", \"no\", \"no\", \"yes\", \"no\", \"yes\", \"yes\", \"yes\")\n\nAlternatively, we could have encoded access to coast as binary values 0 and 1, but as we’ll see shortly, when we have non-numeric variables in a regression model, R will recognize such variables as factors.\nWe will add these values to our dat dataframe and name the new variable Coast:\n\ndat$Coast <- x3\n\nThe way a categorical variable such as Coast is treated in a regression model is by converting categories to binary values. In our example, the variable Coast has two categories, yes and no, which means that one will be coded 0 and the other 1. You could specify which is to be coded 0 or 1, or you can let R do this for you. Let’s run the regression analysis with our new variable (we will not use the variable Professional since we concluded earlier that the variable was redundant).\n\nM3 <- lm( Income ~ Education + Coast, dat = dat)\n\nYou may see a warning message indicating that the variable Coast was converted to a factor (i.e. it was treated as a categorical value). Conversely, if you want to explicitly tell R that the variable Coast should be treated as a category, you can enclose that variable in the as.factor() function as follows:\n\nM3 <- lm( Income ~ Education + as.factor(Coast), dat = dat)\n\nThe as.factor option is important if your categorical variable is numeric since R would treat such variable as numeric.\nWe can view the full summary as follows:\n\nsummary(M3)\n\n\nCall:\nlm(formula = Income ~ Education + as.factor(Coast), data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3054.9  -548.7   277.0   754.5  2211.3 \n\nCoefficients:\n                    Estimate Std. Error t value   Pr(>|t|)    \n(Intercept)          11861.6     1621.8   7.314 0.00000588 ***\nEducation            53284.9     7882.2   6.760 0.00001341 ***\nas.factor(Coast)yes   -671.7     1054.1  -0.637      0.535    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1464 on 13 degrees of freedom\nMultiple R-squared:  0.8643,    Adjusted R-squared:  0.8434 \nF-statistic: 41.39 on 2 and 13 DF,  p-value: 0.000002303\n\n\nYou’ll note that the regression coefficient for Coast is displayed as the variable Coastyes, in other words R chose one of the categories, yes to be coded as 1 for us. The regression model is thus:\n\\(\\widehat{Income}\\) = 11862 + 53285 \\(Education\\) -672 \\(Coast_{yes}\\)\nThe last variable is treated as follows: if the county is on the coast, then Coast takes on a value of 1, if not, it takes on a value of 0. For example, Androscoggin county which is not on the coast is expressed as follows where the variable Coast takes on the value of 0:\n\\(\\widehat{Income}\\) = 11862 + 53285 (23663) -672 (0)\nThe equation for Cumberland county which is on the coast is expressed as follows where the variable Coast takes on the value of 1:\n\\(\\widehat{Income}\\) = 11862 + 53285 (32277) -672 (1)\nTurning our attention to the model summary output, one notices a relatively high \\(P\\)-value associated with the Coast regression coefficient. This indicates that \\(\\beta_2\\) is not significantly different from \\(0\\). In fact, comparing this model with model M1 using anova() indicates that adding the Coast variable does not significantly improve our prediction of \\(Y\\) (income).\n\nanova(M1, M3)\n\nAnalysis of Variance Table\n\nModel 1: Income ~ Education\nModel 2: Income ~ Education + as.factor(Coast)\n  Res.Df      RSS Df Sum of Sq     F Pr(>F)\n1     14 28724435                          \n2     13 27854540  1    869895 0.406 0.5351\n\n\nNote the large \\(P\\) value of 0.54 indicating that \\(\\beta_{coast}\\) does not improve the prediction of \\(Y\\)."
  },
  {
    "objectID": "regression.html#multi-category-variable",
    "href": "regression.html#multi-category-variable",
    "title": "Regression analysis (OLS method)",
    "section": "4.2 Multi-category variable",
    "text": "4.2 Multi-category variable\nFor each category in a categorical variable, there are \\((number\\; of\\; categories\\; -1)\\) regression variables added to the model. In the previous example, the variable Coast had two categories, yes and no, therefore one regression parameter was added to the model. If the categorical variable has three categories, then two regression parameters are added to the model. For example, let’s split the state of Maine into three zones–east, central and west–we end up with the following explanatory variable:\n\nx4 <- c(\"W\", \"C\", \"W\", \"W\", \"E\", \"C\", \"C\", \"C\", \"W\", \"C\",\n        \"C\", \"C\", \"W\", \"E\", \"E\", \"W\")\n\nWe’ll add this fourth predictor variable we’ll call Geo to our dataset dat:\n\ndat$Geo <- x4\n\nOur table now looks like this:\n\n\n\n\n\nIncome\nEducation\nProfessional\nCoast\nGeo\n\n\n\n\nAndroscoggin\n23663\n0.19\n0.079\nno\nW\n\n\nAroostook\n20659\n0.16\n0.062\nno\nC\n\n\nCumberland\n32277\n0.40\n0.116\nyes\nW\n\n\nFranklin\n21595\n0.24\n0.055\nno\nW\n\n\nHancock\n27227\n0.31\n0.103\nyes\nE\n\n\nKennebec\n25023\n0.24\n0.078\nno\nC\n\n\nKnox\n26504\n0.28\n0.089\nyes\nC\n\n\nLincoln\n28741\n0.31\n0.079\nyes\nC\n\n\nOxford\n21735\n0.18\n0.067\nno\nW\n\n\nPenobscot\n23366\n0.23\n0.073\nno\nC\n\n\nPiscataquis\n20871\n0.17\n0.054\nno\nC\n\n\nSagadahoc\n28370\n0.31\n0.094\nyes\nC\n\n\nSomerset\n21105\n0.15\n0.061\nno\nW\n\n\nWaldo\n22706\n0.25\n0.072\nyes\nE\n\n\nWashington\n19527\n0.19\n0.038\nyes\nE\n\n\nYork\n28321\n0.28\n0.084\nyes\nW\n\n\n\n\nNow let’s run a fourth regression analysis using the variables Education and Geo:\n\nM4 <- lm(Income ~ Education + Geo, dat = dat)\n\nAgain, you might see a warning indicating that the variable Geo was flagged as a categorical variable and thus converted to a factor.\nLet’s look at the model summary.\n\nsummary(M4)\n\n\nCall:\nlm(formula = Income ~ Education + Geo, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3187.7  -473.2     1.9   642.7  1527.1 \n\nCoefficients:\n            Estimate Std. Error t value    Pr(>|t|)    \n(Intercept)  12579.3     1217.9  10.329 0.000000252 ***\nEducation    50281.5     4630.6  10.858 0.000000146 ***\nGeoE         -1996.4      854.1  -2.337      0.0376 *  \nGeoW           135.8      688.2   0.197      0.8469    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1237 on 12 degrees of freedom\nMultiple R-squared:  0.9106,    Adjusted R-squared:  0.8882 \nF-statistic: 40.72 on 3 and 12 DF,  p-value: 0.000001443\n\n\nAs expected, the model added two regression parameters, one is called GeoE for counties identified as being on the east side of the state and GeoW for the counties identified as being on the west side of the state. Here are three examples showing how the equation is to be interpreted with different east/central/west designations.\nFor Androscoggin county, its location lies to the west, therefore the variable GeoE is assigned a value of 0 and GeoW is assigned a value of 1:\n\\(\\widehat{Income}\\) = 12579 + 50282 (23663) -1996 (0) + 136 (1)\nFor Hancock county, its location lies to the east, therefore the variable GeoE is assigned a value of 1 and GeoW is assigned a value of 0:\n\\(\\widehat{Income}\\) = 12579 + 50282 (27227) -1996 (1) + 136 (0)\nFor Kennebec county, its location lies in the center of the state, therefore both variables GeoE and GeoW are assigned a value of 0:\n\\(\\widehat{Income}\\) = 12579 + 50282 (25023) -1996 (0) + 136 (0)\nTurning to the model summary results, it appears that one of the new regression coefficients, \\(\\beta_{GeoE}\\) is significantly different from 0 whereas that of \\(\\beta_{GeoW}\\) is not. This seems to suggest that Eastern counties might differ from other counties when it comes to per-capita income. We can aggregate the western and central counties into a single category and end up with a two category variable, i.e one where a county is either on the eastern side of the state or is not. We will use the recode() function to reclassify the values and add the new codes to our dat dataframe as a new variable we’ll call East.\n\ndat$East <- recode(dat$Geo, \" 'W' = 'No' ; 'C' = 'No' ; 'E' = 'Yes' \")\n\nLet’s look at our augmented table.\n\n\n\n\n\nIncome\nEducation\nProfessional\nCoast\nGeo\nEast\n\n\n\n\nAndroscoggin\n23663\n0.19\n0.079\nno\nW\nNo\n\n\nAroostook\n20659\n0.16\n0.062\nno\nC\nNo\n\n\nCumberland\n32277\n0.40\n0.116\nyes\nW\nNo\n\n\nFranklin\n21595\n0.24\n0.055\nno\nW\nNo\n\n\nHancock\n27227\n0.31\n0.103\nyes\nE\nYes\n\n\nKennebec\n25023\n0.24\n0.078\nno\nC\nNo\n\n\nKnox\n26504\n0.28\n0.089\nyes\nC\nNo\n\n\nLincoln\n28741\n0.31\n0.079\nyes\nC\nNo\n\n\nOxford\n21735\n0.18\n0.067\nno\nW\nNo\n\n\nPenobscot\n23366\n0.23\n0.073\nno\nC\nNo\n\n\nPiscataquis\n20871\n0.17\n0.054\nno\nC\nNo\n\n\nSagadahoc\n28370\n0.31\n0.094\nyes\nC\nNo\n\n\nSomerset\n21105\n0.15\n0.061\nno\nW\nNo\n\n\nWaldo\n22706\n0.25\n0.072\nyes\nE\nYes\n\n\nWashington\n19527\n0.19\n0.038\nyes\nE\nYes\n\n\nYork\n28321\n0.28\n0.084\nyes\nW\nNo\n\n\n\n\nAt this point, we could run a new regression with this variable, but before we do, we will control which category from the variable East will be assigned the reference category in the regression model as opposed to letting R decide for us. To complete this task, we will use the function factor().\n\ndat$East <- factor(dat$East, levels = c(\"No\", \"Yes\"))\n\nThis line of code does three things: it explicitly defines the variable East as a factor, it explicitly defines the two categories, No and Yes, and it sets the order of these categories (based on the order of the categories defined in the level option). The latter is important since the regression function lm will assign the first category in the level as the reference.\nWe are now ready to run the regression analysis\n\nM5 <- lm(Income ~ Education + East, dat)\n\nLet’s view the regression model summary.\n\nsummary(M5)\n\n\nCall:\nlm(formula = Income ~ Education + East, data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3114.59  -454.21     5.88   614.49  1600.85 \n\nCoefficients:\n            Estimate Std. Error t value     Pr(>|t|)    \n(Intercept)  12646.2     1125.6  11.235 0.0000000459 ***\nEducation    50264.0     4455.3  11.282 0.0000000437 ***\nEastYes      -2058.9      763.3  -2.697       0.0183 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1190 on 13 degrees of freedom\nMultiple R-squared:  0.9103,    Adjusted R-squared:  0.8965 \nF-statistic: 65.93 on 2 and 13 DF,  p-value: 0.0000001564\n\n\nThis model is an improvement over the model M1 in that a bit more of the variance in \\(Y\\) is accounted for in M5 as gleaned from the higher \\(R^2\\) value. We also note that all regression coefficients are significantly different from 0. This is a good thing. The negative regression coefficient \\(\\hat {\\beta_2}\\) tells us that income is negatively related to the East-other designation. In other words, income tends to be less in eastern counties than in other counties.\nNow let’s look at the lm summary plots to see if the assumptions pertaining to the residuals are met:\n\nOP <- par(mfrow = c(2,2))\nplot(M5, which = c(1:4))\npar(OP)\n\n\n\n\nYou may recall back when we looked at the residuals for model M1 that the Q-Q plot showed residuals near the tails of the distribution that diverged from the theoretical distribution. It appears that this divergence near the ends of the distribution is under control–that’s a good thing. There is also no strong evidence of heteroscadisticy and the Cooks’ Distance indicates that there is no disproportionately influential points (all values are less than 1).\nBear in mind that the geographic designation of East vs. Others may have more to do with factors associated with the two eastern most counties than actual geographic location. It may well be that our variable East is nothing more than a latent variable, but one that may shed some valuable insight into the prediction of \\(Y\\)."
  },
  {
    "objectID": "z_t_tests.html",
    "href": "z_t_tests.html",
    "title": "Comparing means: z and t tests",
    "section": "",
    "text": "Last modified on 2023-04-06"
  },
  {
    "objectID": "z_t_tests.html#the-null-and-the-alternative-hypotheses",
    "href": "z_t_tests.html#the-null-and-the-alternative-hypotheses",
    "title": "Comparing means: z and t tests",
    "section": "1.1 The null and the alternative hypotheses",
    "text": "1.1 The null and the alternative hypotheses\nThe objective of hypothesis testing is to assess whether the observed data are consistent with a well specified (hypothesized) random process, \\(H_o\\). Note that when we mean random here we are not inferring complete randomness but some random variation about a central value. Freedman et al. (2007) define The null and alternative hypotheses as follows:\n\nThe null hypothesis corresponds to the idea that an observed difference is due to chance… The alternative hypothesis corresponds to the idea that the observed difference is real.\n\n\\(Ho\\) is a statement about the true nature of things. To assess whether our observed data are consistent with our null hypothesis we seek to compare our data with the hypothesized value .\n\n\n\n\n\nIn essence, we compare our observed data (usually as a statistical summary) to the hypothesized distribution using a test statistic from which a test of significance is calculated (this tells us how likely our observed statistic agrees with our hypothesized process). These, and derived concepts, are highlighted in the subsequent sections."
  },
  {
    "objectID": "z_t_tests.html#test-statistics",
    "href": "z_t_tests.html#test-statistics",
    "title": "Comparing means: z and t tests",
    "section": "1.2 Test statistics",
    "text": "1.2 Test statistics\nA test statistic is a numerical summary of the data that is compared to what would be expected under the null hypothesis. Test statistics can take on many forms such as the z-tests (usually used for large datasets) or t-tests (usually used when datasets are small)."
  },
  {
    "objectID": "z_t_tests.html#z-tests",
    "href": "z_t_tests.html#z-tests",
    "title": "Comparing means: z and t tests",
    "section": "1.3 \\(z\\)-tests",
    "text": "1.3 \\(z\\)-tests\nThe \\(z\\)-statistic is a measure of how much an observed statistic differs from an expected statistic put forward by the null hypothesis. It is computed as\n\\[\nz = \\frac{observed - expected}{SE}\n\\]\nIn computing the \\(z\\)-statistic, the \\(SE\\) used is not the standard error of the observed data, but the standard error for the null. To be more precise, the \\(SE\\) in this formula is computed from the null’s \\(SD\\), if given. However, in many cases (such as in this working example) the null’s \\(SD\\) can only be estimated from the observed data’s \\(SD\\).\nFor example, the \\(z\\)-statistic for our scenario is:\n\nHo     <- 28.5\nz      <- (mean.x - Ho) / SE.x\n\nwhere SE.x is the observed sample’s standard error. In our working example, \\(z\\)’s value of 2.74 indicates that the sample mean is 2.74 \\(SE\\) ’s away from the hypothesized value.\n\n\n\n\n\nThe red line shows where our observed statistic (mean number of TV hours watched) lies on the hypothesized distribution of mean values defined by \\(H_o\\). The grey dashed lines represent 1 \\(SE\\) to the left and to the right of the hypothesized mean. Each tic mark interval represents a standard deviation. From the graph, it appears that the observed mean value of 30.14 is almost 3 \\(SE\\)’s away from the hypothesized mean of 28.5. If we were to draw many samples from the population described by \\(H_o\\) (i.e. a population whose mean number of TV hours watched equals 28.5), only a small fraction of the sample means would produce a \\(z\\)-value more extreme than the one calculated from our dataset. The area shaded in light red highlights the probability of \\(z\\)-values being more extreme than the one computed from our sample. We can quantify this probability \\(P\\) by looking up its value in a Normal distribution table (the old fashion way), or more simply, by passing the \\(z\\) parameter 2.74 to the R function pnorm.\nThe pnorm function gives us the probability of a \\(z\\) value “greater than” or “less than” the \\(z\\) value computed from our data. In this case, we are interested in knowing the probability of having \\(z\\) values more extreme than the one computed. Since our \\(z\\) value is on the right side of the distribution curve, we will invoke the option lower.tail=FALSE to ensure that the probability returned is for the right tail-end section of the distribution (the pink area in the preceding figure).\n\nP.Ho  <- pnorm(z, lower.tail=FALSE)\nP.Ho\n\n[1] 0.003071959\n\n\nHere, \\(z\\) is on the right side of the curve and the probability of getting a test statistic more extreme than our \\(z\\) is about 0.003 or 0.31% . \\(P\\) is called the observed significance level and is sometimes referred to as the \\(P\\)-value. The smaller this probability, the stronger the evidence against \\(Ho\\) meaning that the odds of the mean TV hours watched per household being 28.5 is very small. Careful, \\(P\\) is not the chance of \\(Ho\\) being right, such statement is prevalent but is wrong.\nSometimes researchers will define a \\(P\\) value for which \\(Ho\\) will be rejected. Such value is usually referred to as the \\(\\pmb{\\alpha \\; value}\\). If such a test is requested, we must determine if the test is one-tailed or two-tailed. A test is one-tailed if the alternate hypothesis, \\(H_a\\), is of the form “greater than” or “less than” (e.g. the mean number of TV hours watched are greater than 28.5). A test is two-tailed if \\(H_a\\) is not of the form “greater than” or “less than” (e.g. the mean number of TV hours watched differs from 28.5). Here are a few examples:\n\nIf we chose an \\(\\alpha\\) value of 0.05 and we wanted to test the hypothesis that our observed mean hours is different than \\(H_o\\) we would define a two-tailed test, meaning that we would have to define rejection regions associated with \\(P\\) values of less than 0.025 and greater than 0.975 (or \\(z\\) values of -1.96 and 1.96 respectively). The reason we choose \\(p\\) values of 0.025/0.975 and not 0.05/0.95 is because we need to split the 0.05 \\(\\alpha\\) value across both tails of the curve (remember that we are rejecting the null if our \\(z\\) value falls in either tails of the curve).\n\n\n\n\n\n\n\nIf we chose an \\(\\alpha\\) value of 0.05 and we wanted to test the hypothesis that our observed mean hours is greater than \\(H_o\\), we would define a one-tailed test, meaning that we would have to define a rejection region associated with a \\(P\\) value greater than 0.95.\n\n\n\n\n\n\nIn our working example, if we had chosen a two-tailed test, we would reject the null at a 5% and 1% \\(\\alpha\\) value (these would represent rejection regions associated with \\(z\\) values of +/- 1.96 and +/- 2.58 respectively). However, if our \\(\\alpha\\) value was set at 0.005 (0.5%), we could not reject the null hypothesis since the \\(z\\)-value associated with a two-tailed test for \\(\\alpha\\)=0.005 is 2.81 (greater than our observed \\(z\\) value of 2.74).\nThe following table highlights popular \\(\\alpha\\) values and their associated \\(z\\) values for a one-tailed and two-tailed test:\n\n\n\n\n\\(\\small\\alpha=0.1\\)\n\\(\\small\\alpha=0.05\\)\n\\(\\small\\alpha=0.01\\)\n\\(\\small\\alpha=0.005\\)\n\n\n\n\n1-tailed\n-1.28 or 1.28\n-1.645 or 1.645\n-2.33 or 2.33\n-2.58 or 2.58\n\n\n2-tailed\n-1.645 & 1.645\n-1.96 & 1.96\n-2.58 & 2.58\n-2.81 & 2.81\n\n\n\nNote that the one-tailed test requires that only one condition be met (only one of the rejection regions is of concern) whereas a two-tailed test requires that two conditions be met (both rejection regions are of concern).\nIt’s important to note that the \\(z\\)-test makes some restrictive assumptions: * the sample size is reasonably large * the normal (Gaussian) distribution can be used to approximate the distribution of the sample statistic (e.g. the mean) being investigated.\nAn alternative to the \\(z\\)-test, the \\(t\\)-test, is discussed in the following section."
  },
  {
    "objectID": "z_t_tests.html#t-tests",
    "href": "z_t_tests.html#t-tests",
    "title": "Comparing means: z and t tests",
    "section": "1.4 \\(t\\)-tests",
    "text": "1.4 \\(t\\)-tests\nWhen working with small sample sizes (typically less than 30), the \\(z\\)-test has to be modified. For starters, the shape of the sampling distribution (i.e. the distribution of means one would compute from many different samples from the same underlying population) now depends on the shape of the underlying population distribution which must therefore be approximately normal in shape. These requirements can be quite restrictive because in most cases we do not know the population’s distribution.\nContinuing with our working example, let’s assume that instead of a sample size of 50 we now have a sample size of 10.\n\nx2 <- c(37.13, 32.02, 26.05, 31.76, 31.90, 38.62, 21.63, 40.75, 31.36, 27.01)\n\nNext we compute the mean and standard deviation of the sample. Note that the standard deviation can be computed in one of two ways: \\(TSS/\\sqrt{n}\\) or \\(TSS/\\sqrt{n-1}\\) where \\(TSS\\) is the total sum of squares, \\(\\sum{(x - \\bar{x})^2}\\), and \\(n\\) is the sample size. The latter formulation of \\(sd\\) (i.e. the one with the \\(\\sqrt{n-1}\\) in the denominator) is recommended for small sample sizes but can be used for large sample sizes as well. R’s sd() function is computed using the \\(\\sqrt{(n-1)}\\) denominator. This is what we want to use with our small sample.\n\nmean.x2 <- mean(x2)\nsd.x2   <- sd(x2) \n\nNext, we compute the standard error. Recall that the standard error tells us something about the confidence in the range of mean values (or some other statistic) for the underlying population based on our sample; it’s not a measure of spread of our sample data.\n\nSE.x2 <- sd(x2) / sqrt(length(x2)) \n\nThe next step is to find the \\(P\\)-value. When working with large sample sizes, the normal (Gaussian) distribution curve does a good job in approximating the distribution of a sample statistic (such as the mean). It does not, however, do a good job in approximating the distribution of that same statistic when these are computed from small sample sizes.\n\n\n\n\n\nThe black line represents the distribution of the mean hours (from many hypothetical samples) of TV watched as approximated by the normal curve. The red line represents the same parameter but represented this time by a student distribution curve with a degree of freedom, \\(df\\), of 4. The \\(df\\) is computed by subtracting 1 from the total number of data points in the sample.\nBy convention when computing a test statistic for small sample sizes (and when a student curve is used), we refer to the test statistic as the \\(t\\) statistic (or \\(t\\) value). For our dataset \\(t\\) can be computed as follows:\n\nt.val <- (mean.x2 - Ho) / SE.x2\n\nHere, we make a point not to name the \\(t\\) value t in our code since R has a function with the same name, t() (a matrix transpose function). Had we named our variable t, than that variable name would have masked the internal function t(). This would not have been a big deal for our working example since we won’t be using the transpose function, but it’s good practice to use variables not already in use in R.\nThe next step is to compute the \\(P\\)-value. We will compute \\(P\\) using both the normal distribution curve (which we normally do for a large sample) and Student’s curve (which is recommended for a small sample).\n\nP.Ho.norm <- pnorm(t.val, lower.tail=FALSE)\nP.Ho.stud <- pt(t.val, df = length(x2) - 1, lower.tail = FALSE)\n\nThe \\(P\\)-value computed from the normal curve is 0.038 and that computed from Student’s curve is 0.055. If an \\(\\alpha\\) value of 5% was requested, the normal distribution approximation would suggest that chance variation alone could not explain the discrepancy between our hypothesized mean number of TV hours watched and that computed from our sample. Yet, if Student’s approximation of the distribution is used (as it should be with the small sample), we could not reject the null (at least not at the 5% significance level).\nThe following figure summarizes the decision tree one should follow in deciding which curve to use when calculating a \\(P\\)-value. This figure is adapted from Freedman et al. (p. 493).\n\n\n\n\n\nMany textbooks only cover the \\(t\\)-test and not the \\(z\\)-test. In fact , you may not find a \\(z\\)-test implementation in some statistical software. This is because the \\(t\\)-test results will converge with the \\(z\\)-test results as the sample size gets larger."
  },
  {
    "objectID": "z_t_tests.html#follow-up-examples",
    "href": "z_t_tests.html#follow-up-examples",
    "title": "Comparing means: z and t tests",
    "section": "1.5 Follow-up examples",
    "text": "1.5 Follow-up examples\n\n1.5.1 Problem 1\nYou conduct a survey where the respondent is to answer yes or no to a question. Of the 45 respondents, 20 answer yes. You want to know if the percentage of yes’ is significantly different from an expected value of 50%.\n\n\n1.5.2 Solution to problem 1\nThe data can be treated as a binomial proportion where the fraction of yes’ in our sample is \\(\\hat{p} = 20/45 = 0.444\\) and the fraction of no’s is \\(\\hat{q} = 1 - \\hat{p} = 1 - 0.444 = 0.555\\) (the \\(\\hat{ }\\) symbol reminds us that these are estimate fractions of the true yes’ and no’s in the overall population). The hypothesis \\(H_o\\) is that the number of yes’ in the population equals the number of no’s or \\(p_o = q_o = 0.5\\).\nWe first need to compute \\(SE_{Ho}\\) (The standard error for the null and not the observed data). In this example, the standard deviation of the null hypothesis is given to us since we are told what the proportion of yes’ and no’s should be equal to in the hypothesized population (i.e. we know that the hypothesized population has 22.5 yes’ and 22.5 no’s). If we could not glean a standard deviation from the null, we would therefore have to rely on the sample’s \\(SD\\) instead. \\(SE_{Ho}\\) can be computed as follows.\n\\(SE_{Ho} = \\sqrt{ \\frac{\\displaystyle (fraction\\; of\\; yes')(fraction\\; of\\; no's)}{\\displaystyle n}}=\\sqrt{\\frac{\\displaystyle (0.5)(0.5)}{\\displaystyle 45}}=0.075\\).\nA way to interpret \\(SE_{Ho}\\) is to say that if many surveys of 45 people were collected from a population defined by the null (i.e. a population where the number of yes’ equals the number of no’s), 68% of the fraction of yes’ would fall between \\(q_o - 1SE_{Ho}\\) and \\(q_o + 1SE_{Ho}\\) or between the interval defined by the fractions (0.425, 0.575) or (42.5%, 57.5%).\nNext, we compute the test statistic, \\(z\\):\n\\(z = (\\hat{p} - p_o)/SE_{Ho} = (.444 - 0.5)/0.075 = -0.75\\),\nThe observed fraction of yes’ is 0.75 \\(SE\\)’s below the expected count of 22.5 (or 0.5 yes’).\n\n\n\n\n\nSince the sample size is relatively large, we can compute the \\(P\\)-value using the normal curve approximation using the function pnorm(0.75, lower.tail=FALSE) = 0.2266274.\nThe entire R analysis for this example can be completed as follows:\n\nn   <- 45                        # number of individuals surveyed\np   <- 0.444                     # The fraction of yes'\nq   <- 1 - p                     # The fraction of no's\nho  <- 0.5                       # The null hypothesis\nSEo <- sqrt(0.5 * 0.5 / n)       # The standard error for Ho sample distribution\nz   <- ( p - ho ) / SEo          # The z-statistic\nP.z <- pnorm(z, lower.tail=TRUE) # Get the probability\n\nNote that in the last line of code we set lower.tail to TRUE since we are interested in the portion of the curve to the left of our test statistic. Had \\(z\\) turned out positive (indicating that our observed fraction of yes’ is greater than expected under the null), we would have focused on right tail of the curve (i.e. setting lower.tail to FALSE).\nHence, there is a 23% chance that the fraction of yes’ one could expect to measure under \\(H_o\\) could be more extreme than the one observed or, put differently, if 100 investigators were to conduct their own survey of 45 people from a population whose number of yes’ equals the number of no’s, 23 of the survey results would have a percentage of yes’ more extreme than ours. So our observed fraction of yes’ is consistent with what one would expect under the null hypothesis. It would probably be safe then to state that we cannot reject the null hypothesis.\n\n\n1.5.3 Problem 2\nAldicarb, a pesticide substance, is measured from a sampling well 4 times over the course of a season. The concentrations measured are 6.3, 7.1, 5.8 and 6.9 ppb (parts per billion). The maximum contaminant level (MCL) has been set at 7 ppb. We want to determine if the average concentrations observed are less than the MCL of 7 ppb at the 99% significance level.\n\n\n1.5.4 Solution to problem 2\nThe question asks if the true mean concentration (from the four measurements) is less than or equal to 7 ppb. We do not have the luxury of monitoring this concentration continuously so we must contend with just four observations. These four observations are random variables that can be expected to fluctuate around the true mean concentration of the well (a value we are not privy to). We are hypothesizing that the true mean concentration is less than 7 ppb. We want to compare our observed mean concentration to the range of mean concentrations we could expect if we were to sample (with four concentration measurements) from a well whose mean concentration is indeed 7 ppb. If our observed concentration is not consistent with what we would expect to measure from a well whose true concentration is 7 ppb, we can than state that the true mean concentration of aldicarb in the well is less than 7 ppb.\nThis problem differs from the last one in that we are interested in a different parameter: the mean (of a concentration) instead of the fraction. It also differs from the last problem in that we are now defining a cutoff probability (aka a rejection region) of 0.99. In the last exercise, we were only seeking the probability of finding a test statistic more extreme than expected under a null (and thus leaving the interpretation of what is significant up to someone else).\nWe need to compute the mean of the observed concentrations, \\(\\hat{\\mu}\\), and the standard error of the mean concentrations we would observe if the null was the true state of things, \\(SE_{Ho}\\). For the latter, we do not know the actual standard deviation of the null distribution (this is another difference between this example and the previous example where the standard deviation–and thus the standard error–was gleaned directly from null). We will therefore assume that \\(SD\\) of the null is the same as \\(SD\\) from our observed data. Note that we must therefore assume that the distribution of aldicarb sample mean concentrations follows a normal curve!\n\\(\\hat{\\mu} = \\frac{\\displaystyle 6.3 + 7.1 + 5.8 + 6.9}{\\displaystyle 4}=6.525\\)\n\\(SE_{Ho} = \\frac{\\displaystyle SD_{wells}}{\\sqrt{ \\displaystyle n}}=\\frac{\\displaystyle 0.591}{\\displaystyle \\sqrt{4}} = 0.295\\).\nNote that we are using the \\(SD\\) formula for small samples (i.e. the one with the \\(\\sqrt{n-1}\\) denominator) which just happens to be what R defaults to.\nThe test statistic, \\(t\\) is computed as follows:\n\\(t = (\\hat{\\mu} - \\mu_o)/SE_{Ho} = (6.525 - 7)/0.295 = -1.6\\),\n\\(t\\) is negative implying that our observed concentration is to the left of the hypothesized value of 7. In this example, a 99% confidence interval implies that our observed mean concentration of 6.5 ppb should be at least 3 \\(SE\\)’s to the left of our MCL of 7 ppb.\n\n\n\n\n\nThis dataset is very small (only 4 observations). This implies that we will want to use the student curve as opposed to the normal curve when we calculate the the \\(P\\)-value.\n\nx   <-  c(6.3, 7.1, 5.8, 6.9)               # the four observed concentrations\nn   <- length(x)                            # number of observations \nmu  <- mean(x)                              # the mean concentration\nho  <- 7                                    # The null hypothesis\nSEo <- sd(x) / sqrt(n)                      # The standard error for Ho sample distribution\nt   <- ( mu - ho ) / SEo                    # The t-statistic\nP.z <- pt(t, df = n - 1, lower.tail = TRUE)# Get the probability\n\nP.z returns a value of 0.10, or 10%. In other words, 10% of the mean concentrations from samples collected from a well whose (hypothesized) mean concentration is 7 ppb could be less than our observed concentration. This implies that our observed concentration could well be from a well whose concentration hovers around 7 ppb. If that’s the case, and we were to collect more samples from the well, we could have values greater than 7 ppb (recall that the curve centered on 7 ppb represents the probability distribution of mean concentrations centered on 7 ppb). Given our \\(P\\) value of 0.1, we cannot reject the null and therefore cannot state that our observed concentration is less than 7 ppb at a 99% significance level."
  },
  {
    "objectID": "z_t_tests.html#using-the-built-in-t.test-function",
    "href": "z_t_tests.html#using-the-built-in-t.test-function",
    "title": "Comparing means: z and t tests",
    "section": "1.6 Using the built in t.test function",
    "text": "1.6 Using the built in t.test function\nThe six or seven lines of code used to compute the \\(t\\)-test can easily be replaced with R’s t.test() function. Using example 2, we can compute the t-test as follows:\n\nt.test(x, mu=7, alternative=\"less\", conf.level= 0.99)\n\nLet’s look at the t.test() parameters. mu is the hypothesized mean value. alternative determines if the test is is two-tailed (=\"two.sided\") or one-tailed (=\"less\" or =\"greater\"). The less option gives use the probability to the left of our test statistic while the greater option gives us the probability to the right of our test statistic. conf.level determines the alpha level set for \\(P\\).\nNow let’s look at the output.\n\n\n\n    One Sample t-test\n\ndata:  x\nt = -1.6077, df = 3, p-value = 0.1031\nalternative hypothesis: true mean is less than 7\n99 percent confidence interval:\n     -Inf 7.866558\nsample estimates:\nmean of x \n    6.525 \n\n\nThe output variables are, for the most part, self-explanatory. The value \\(t\\) is the same as the one computed earlier. The \\(P\\) value here gives us the probability to the left of our test statistic."
  },
  {
    "objectID": "z_t_tests.html#z-test-with-two-samples-large-sample-sizes",
    "href": "z_t_tests.html#z-test-with-two-samples-large-sample-sizes",
    "title": "Comparing means: z and t tests",
    "section": "2.1 \\(z\\) test with two samples (large sample sizes)",
    "text": "2.1 \\(z\\) test with two samples (large sample sizes)\nIf the sample sizes in both samples are large (usually more than 30), then a \\(z\\) test is appropriate. The \\(z\\) statistic is computed as follows: \\[\nz = \\frac{observed\\; difference - expected\\; difference}{SE\\; for\\; difference}\n\\] The standard error for the difference of the two samples is: \\[\nSE = \\sqrt{SE_{sample\\; 1}^2 + SE_{sample\\; 2}^2}\n\\]\n\n2.1.1 Example\nThe National Assessment of Education Progress (NAEP) administered a reading test for 17 year-olds in 1990 and 2004. The average score was 290 and 285 respectively; a slight decrease over the course of 14 years. The standard deviation, \\(SD\\) was 40 and 37 respectively. The sample size for both years was 1000. So this begs the question, was the drop in reading assessment just a chance variation or real?\n[This example is taken from Freedman et al., page 503]\n\n\n2.1.2 Solution\nThe standard error for each sample can be computed as follows: \\[\nSE_{1990} = \\frac{SD_{1990}}{\\sqrt{sample\\; size}}=\\frac{40}{\\sqrt{1000}} = 1.26\n\\] \\[\nSE_{2004} = \\frac{SD_{2000}}{\\sqrt{sample\\; size}}=\\frac{37}{\\sqrt{1000}} = 1.17\n\\] The standard error for the difference can be computed as follows: \\[\nSE = \\sqrt{SE_{1990}^2 + SE_{2004}^2} = \\sqrt{1.26^2 + 1.17^2} = 1.72\n\\] Finally, we can compute \\(z\\) as follows (keeping in mind that the expected difference between both years is our null hypothesis, i.e. no difference, 0): \\[\nz = \\frac{(score_{2004} - score_{1990}) - (expected\\; difference)}{SE\\; of\\; difference}=\\frac{(285-290) - 0}{1.72} = -2.9\n\\] So the difference is 2.9 \\(SE\\)’s below what would be expected if \\(Ho\\) (i.e. no difference between years) was true. Looking up the \\(P\\) value for an \\(SE\\) of 2.9 on a normal curve is 0.002. Put differently, if their was truly no difference in reading scores between both years, then the odds of getting a \\(z\\) score as extreme as the one we just computed is 0.1%. If we had defined an \\(\\alpha\\) confidence value of 5% or even 1%, we could conclude that the difference between both years is real.\nThe following figure illustrates the result.\n\n\n\n\n\nThe black curve encompasses the difference in scores between years one would expect to observe from many samples drawn under the assumption that the difference in scores between both years is only due to chance variability alone. The red vertical line shows where our value lies; far into the left-end tail.\nThe above can be coded in R as follows:\n\nSE.1990 <-  40 / sqrt(1000)               \nSE.2004 <-  37 / sqrt(1000)  \nSE      <- sqrt( SE.1990^2 + SE.2004^2)                        \nz       <- (285 - 290) / SE\nP.z     <- pnorm(z, lower.tail = TRUE) \n\nThe variable z stores the test statistic and the variable P.z stores the probability \\(P\\)."
  },
  {
    "objectID": "z_t_tests.html#t-test-with-two-samples-small-sample-sizes",
    "href": "z_t_tests.html#t-test-with-two-samples-small-sample-sizes",
    "title": "Comparing means: z and t tests",
    "section": "2.2 \\(t\\) test with two samples (small sample sizes )",
    "text": "2.2 \\(t\\) test with two samples (small sample sizes )\nIf the sample sizes in at least one of the two samples is small (usually less than 30), then a \\(t\\) test is appropriate. Note that a \\(t\\) test can also be used with large samples as well, in some cases, statistical packages will only compute a \\(t\\) test and not a \\(z\\) test.\nTo use the \\(t\\)-statistic, the two samples must be:\n\nfrom normally distributed populations,\nfrom populations with equal variances,\nuncensored.\n\nMany times, the aforementioned criteria are not known and may have to be assumed if theory allows.\nIf the assumption of equal variances is met, the standard error can be pooled from both sample’s standard errors. \\[\nSE_{pooled} = \\sqrt{\\frac{(n_1 - 1)SD_1^2+(n_2 - 1)SD_2^2}{n_1 + n_2 - 2}}\n\\]\nSubscripts \\(1\\) and \\(2\\) denote samples 1 and 2. Note that we are using \\(SD_1\\) and \\(SD_2\\) in this equation as opposed to \\(SE_1\\) and \\(SE_2\\). The test statistic \\(t\\) is therefore computed as follows:\n\\[\nt = \\frac{(observed\\; difference - expected\\; difference) - hypothesized\\; difference}\n         {SE_{pooled}\\sqrt{1/n_1 + 1/n_2}}\n\\]\nThe \\(t\\) value is then used to look up the \\(P\\) value on a Student curve using \\(n_1 + n_2 - 2\\) degrees of freedom.\n\n2.2.1 Example\nGroundwater sulfate concentrations are monitored at a contaminated site over the course of a year. Those concentrations are compared to ones measured at background sites for the same time period. You want to determine if the concentration at the contaminated site is significantly larger than that for the background site. The concentrations of sulfate (in ppm) for both sites are as follows:\n\n\n\nContaminated\nBackground\n\n\n\n\n600\n560\n\n\n590\n550\n\n\n570\n570\n\n\n570\n550\n\n\n565\n570\n\n\n580\n590\n\n\n\n550\n\n\n\n580\n\n\n\n\n\n2.2.2 Solution\nYou will setup this problem as follows: the null hypothesis, \\(H_o\\), states that the concentrations between both sites is the same; the alternative hypothesis, \\(H_a\\), states that the contaminated site has a concentration greater than the background.\nWe will reference the contaminated site with the \\(1\\) subscript and the background site with the subscript \\(2\\). The means and standard deviations are \\(\\mu_1 = 579.2\\), \\(\\mu_2 = 565\\), \\(SD_1 = 13.6\\) and \\(SD_2 = 15.1\\).\nThe pooled standard error of the mean is therefore: \\[\nSE_{pooled} = \\sqrt{\\frac{(6 - 1)13.6^2+(8 - 1)15.1^2}{6 + 8 - 2}}=14.5\n\\]\nand \\(t\\) is: \\[\nt = \\frac{(579.2-565) - 0)}{14.5\\sqrt{1/6 + 1/8}} = 1.8\n\\]\nOne \\(SE\\) is \\(14.5\\sqrt{1/6 + 1/8}=\\) 7.8 ppm, therefore our observed difference of 14.2 ppm (computed from 579.2 - 565) is 1.8 \\(SE\\) from the expected difference of 0 (recall that for a \\(t\\)-test \\(SE\\) equals \\(SE_{pooled}\\sqrt{1/n_1 + 1/n_2}\\)). Looking up the \\(P\\) value on a Student curve gives us a probability of 0.048 (i.e. there is a 4.8% chance that, assuming there is no difference in concentrations between sites, the difference between means would be greater than that observed by chance variation alone).\n\n\n\n\n\nThe following block of code runs the entire analysis. Here, the data are entered manually. If data are loaded from a file into a data frame, you can set the variables s1 and s2 equal to the pertinent columns in the data frame.\n\ns1 <- c(600, 590, 570, 570, 565, 580)             # Contaminated site\ns2 <- c(560, 550, 570, 550, 570, 590, 550, 580)   # Background sites\n\nSD1 <- sd(s1)\nSD2 <- sd(s2)\nSE  <- sqrt(((length(s1) - 1) * SD1^2 + (length(s2) - 1) * SD2^2)/(length(s1) + length(s2) - 2))\nt   <- ((mean(s1) - mean(s2)) - 0) / (SE * sqrt(1/length(s1) + 1/length(s2)))\nP   <- pt(t, df = length(s1) + length(s2) - 2, lower.tail = FALSE)\n\nThe variable P stores the probability value 0.0477 and variable t stores the value 1.81.\nYou can use the t.test function if the data are used to compute the \\(t\\) test. If you only have the means, standard deviations and sample sizes at your disposal (and not the raw data), you must compute \\(t\\) and \\(P\\) as shown in the last code block. Since we have the original concentrations for both the contaminated and background sites in this example, we can run the t.test as follows:\n\nt.test(s1, s2, var.equal=TRUE, alternative = \"greater\")\n\nThis returns the following values:\n\n\n\n    Two Sample t-test\n\ndata:  s1 and s2\nt = 1.8099, df = 12, p-value = 0.04771\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n 0.2157541       Inf\nsample estimates:\nmean of x mean of y \n 579.1667  565.0000 \n\n\nNote that the t.test function can also be used to run a \\(z\\)-test between means if tabulated data are available."
  },
  {
    "objectID": "z_t_tests.html#notes-on-z-and-t-tests",
    "href": "z_t_tests.html#notes-on-z-and-t-tests",
    "title": "Comparing means: z and t tests",
    "section": "2.3 Notes on z and t tests",
    "text": "2.3 Notes on z and t tests\n\n2.3.1 Assumption of equal variance\nOne assumption that needs to be met when conducting a test of significance with small sample sizes is the equality of variances between samples. This is an assumption we have made thus far in our working examples. But such an assumption may not be tenable in real life. If such is the case, you may want to resort to alternate techniques such as robust test of significance techniques. However, if you are working with the raw data, you can use the t.test function along with the parameter var.equal=FALSE option set. This option invokes Welch’s variance approximation which is believed to provide a more robust \\(t\\)-test analysis. Working with the last example, we can compute the \\(t\\)-test using Welch’s approximation as demonstrated in the following line of code.\n\nt.test(s1, s2, var.equal=FALSE, alternative = \"greater\")\n\nNote the var.equal=FALSE option (this is the default option if not explicitly defined). The output of this function is shown below.\n\n\n\n    Welch Two Sample t-test\n\ndata:  s1 and s2\nt = 1.8402, df = 11.514, p-value = 0.04582\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n 0.3974713       Inf\nsample estimates:\nmean of x mean of y \n 579.1667  565.0000 \n\n\nInterestingly, the Welch’s \\(t\\)-test returns a smaller \\(P\\) value. It must be noted that Welch’s \\(t\\)-test also has its limitations. If in doubt, you might want to revert to more robust tests of significance such as the Wilcoxon rank-sum tests or permutation tests.\nNote that equality of variances between two samples can be tested using the \\(F\\) test.\n\n\n2.3.2 Population distribution and small sample sizes\nIf the sample size is small, the shape of the population distribution will influence the tests. If the population is not normally distributed, the data may need to be transformed prior to conducting a \\(t\\)-test. Many environmental data such as concentrations of “something” tend to be positively skewed. If such is the case, a popular transformation for skewed data is the natural logarithm transformation, log. Note that when you are conducting a \\(t\\) or \\(z\\) test on log transformed data, you are conducting a hypothesis test on the ratio of the medians and not a hypothesis about the difference of the means (Millard et al., p. 416-417).\nFor example, using TcCB concentration data between a background, Ref, and contaminated site, Cont (Millard et al., p.420),\n\nRef <-  c(0.22,0.23,0.26,0.27,0.28,0.28,0.29,0.33,0.34,0.35,0.38,0.39,\n        0.39,0.42,0.42,0.43,0.45,0.46,0.48,0.5,0.5,0.51,0.52,0.54,\n        0.56,0.56,0.57,0.57,0.6,0.62,0.63,0.67,0.69,0.72,0.74,0.76,\n        0.79,0.81,0.82,0.84,0.89,1.11,1.13,1.14,1.14,1.2,1.33)\nCont <- c(0.09,0.09,0.09,0.12,0.12,0.14,0.16,0.17,0.17,0.17,0.18,0.19,\n        0.2,0.2,0.21,0.21,0.22,0.22,0.22,0.23,0.24,0.25,0.25,0.25,\n        0.25,0.26,0.28,0.28,0.29,0.31,0.33,0.33,0.33,0.34,0.37,0.38,\n        0.39,0.4,0.43,0.43,0.47,0.48,0.48,0.49,0.51,0.51,0.54,0.6,\n        0.61,0.62,0.75,0.82,0.85,0.92,0.94,1.05,1.1,1.1,1.19,1.22,\n        1.33,1.39,1.39,1.52,1.53,1.73,2.35,2.46,2.59,2.61,3.06,3.29,\n        5.56,6.61,18.4,51.97,168.64)\n\nwe can test the null hypothesis that the concentrations between both wells are equal using the raw (un-transformed) data (to be conservative, we will assume that the variances are not equal and invoke Welch’s assumption about the variance),\n\nt.test(Cont,Ref,alternative=\"greater\", var.equal=FALSE)\n\nwhich gives us the following:\n\n\n\n    Welch Two Sample t-test\n\ndata:  Cont and Ref\nt = 1.4538, df = 76.05, p-value = 0.07506\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n -0.4821023        Inf\nsample estimates:\nmean of x mean of y \n3.9151948 0.5985106 \n\n\nThe test statitic of 1.45 indicates that there is a 7.5% chance that we could see a test statistic more extreme under \\(H_o\\) than the one computed.\nIf we log-transform the data, we get\n\nt.test(log(Cont),log(Ref),alternative=\"greater\", var.equal=FALSE)\n\nwhich gives us the following:\n\n\n\n    Welch Two Sample t-test\n\ndata:  log(Cont) and log(Ref)\nt = 0.42589, df = 101.99, p-value = 0.3355\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n -0.2090447        Inf\nsample estimates:\n mean of x  mean of y \n-0.5474262 -0.6195712 \n\n\nNotice the larger \\(P\\) value of %33.5 which indicates that we should not reject the null and that for all intents and purposes, we cannot dismiss the chance that the differences in median concentrations (as expressed by a ratio) are due to variability alone. Remember that by log-transforming the data we are looking at the ratio of the medians, not the difference between means, so interpret these results with caution!\nOther methods used to test the normality of the data are tests of skewness and kurtosis.\n\n\n2.3.3 Tests of paired samples (paired t-test)\nSuppose that you want to compare concentrations of ozone between two locations on a set of dates (i.e. for each date, ozone concentrations are compared between two sites). This problem differs from the groundwater sulfate example shown earlier in that a sample from each location is paired. This violates one of the \\(z\\) and \\(t\\) test requirements in that samples from both groups be independent of each other. In the case of the ozone concentration, this assumption does not hold. However, Freedman et al. (page 510) explain that the errors introduced (in estimating \\(SE\\)) when violating this assumption tend to cancel each other out when applied to paired tests. Using data from Millard et al. (page 408) where ozone concentrations for two areas (Yonkers, NY and Stamford, CT) are collected on a daily bases the paired \\(t\\)-test can be computed by invoking the paired = TRUE option.\n\n# Ozone concentrations in ppb)\nyonkers <- c(47,37,45,52,51,22,27,25,55,72,132,106,42,45,80,\n            107,21,50,31,37,19,33,22,45,36,24,88,111,117,31,\n            37,93,106,64,83,97,79,36,51,75,104,107,68,19,67,\n            20,35,30,31,81,119,76,108,85,96,48,60,54,71,50,37,\n            47,71,46,41,49,59,25,45,40,13,46,62,80,39,74,66,\n            82,47,28,44,55,34,60,70,41,96,54,100,44,44,75,86,\n            70,53,36,117,43,27,77,75,87,47,114,66,18,25,14,27,\n            9,16,67,74,74,75,74,42,38,23,50,34,58,35,24,27,17,\n            21,14,32,51,15,21)\nstamford <- c(66,52,49,64,68,26,86,52,75,87,188,103,82,71,103,\n             240,31,40,47,51,31,47,14,71,61,47,196,131,173,37,\n             47,215,230,69,98,125,94,72,72,125,143,192,122,32,\n             114,32,23,71,38,136,169,152,201,134,206,92,101,119,\n             124,83,60,124,142,124,64,75,103,46,68,87,27,73,59,\n             119,64,111,80,68,24,24,82,100,55,91,87,64,170,86,202,\n             71,85,122,155,80,71,28,212,80,24,80,169,174,141,202,\n             113,38,38,28,52,14,38,94,89,99,150,146,113,66,38,80,\n             80,99,71,42,52,33,38,24,61,108,38,28)\nt.test(stamford, yonkers, alternative=\"greater\", var.equal=FALSE, paired=T)\n\n\n    Paired t-test\n\ndata:  stamford and yonkers\nt = 13.044, df = 131, p-value < 0.00000000000000022\nalternative hypothesis: true mean difference is greater than 0\n95 percent confidence interval:\n 30.52863      Inf\nsample estimates:\nmean difference \n        34.9697 \n\n\n\n\n\nThe \\(t\\)-test result can be interpreted in the same way. The test statistic of 13.04 is way up in the right-tail end side of the curve. Its associated probability of nearly 0 indicates that the differences in ozone concentrations between both locations cannot be explained by chance variability alone. Note that even though we invoked the \\(t\\) test the results are identical to what we would have gotten had we performed a \\(z\\) test since the sample size is large."
  }
]
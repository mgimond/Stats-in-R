{"title":"Comparing variances: Fisher's F-test","markdown":{"yaml":{"title":"Comparing variances: Fisher's F-test"},"headingText":"loadfonts(device=\"win\") # TO view in windows","containsRefs":false,"markdown":"\n\n*Last modified on `r Sys.Date()`*\n\n```{r, echo = FALSE, message = FALSE}\nsource(\"libs/Common.R\")\n```\n\n```{r Load fonts, echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE}\nlibrary(extrafont)\nfont_import(prompt=FALSE, pattern=\"[A/a]rchitects\")\nloadfonts(device=\"postscript\")\n```\n\n\n\nThe test of variances requires that the two sampled population be normally distributed and that the samples are randomly selected from their respective populations.\n\n# Introduction\n\nThe method is simple; it consists of taking the ratio between the larger population variance, $\\sigma_1^2$, and the smaller population variance, $\\sigma_2^2$, then looking up the ratio on an $F$-distribution curve. The null hypothesis states that the ratio equal 1,\n$$\nH_o: \\frac{\\sigma_1^2}{\\sigma_2^2} = 1\n$$\nand the alternate hypothesis states that the ratio differs from 1 (i.e. the variances differ),\n$$\nH_a: \\frac{\\sigma_1^2}{\\sigma_2^2} \\neq  1\n$$\nor is greater than 1 (i.e. $\\sigma_1^2$ is significantly bigger than $\\sigma_2^2$),\n$$\nH_a: \\frac{\\sigma_1^2}{\\sigma_2^2} \\gt  1\n$$\n\nSince the larger variance is assigned to the numerator by convention, we do not have a situation where the ratio is less than 1.\n\nSince we are working with samples, we work with the sample variances $s_1^2$ and $s_2^2$ and compute the test statistic $F$ as follows:\n$$\nF = \\frac{s_1^2}{s_2^2}\n$$\n\nThe shape of the $\\pmb F$**-distribution** curve is defined by both sample's $df$'s, i.e. $(n_1-1)$ and $(n_2-1)$. Like the $\\chi^2$ distribution, the $F$ distribution tends to be skewed to the right, especially for large $df$'s.\n\n# Example 1\n\nIn one of the examples in the _$z$ and $t$ test_ section, we seek to compare the concentration of sulfates between a background sites and a contaminated well (data taken from Millard _et al._, p. 418). Did the two samples have equal variances? The table of concentrations is reproduced here.\n\nContaminated      Background\n---------------   -------------------\n600               560\n590               550\n570               570\n570               550\n565               570\n580               590\n                  550\n                  580\n\n## Solution to example 1\nThe variances for both samples are $s_{Ref}^2 = 712.5$ and $s_{Cont}^2 = 336.7$. Since $s_{Ref}^2 > s_{Cont}^2$, the value $s_{Ref}^2$ will be in the numerator giving us the following test statistic:\n\n$$\nF = \\frac{s_{Ref}^2}{s_{Cont}^2} = \\frac{712.5}{336.7} = 2.12\n$$\n\nNext, we must determine where the $F$ statistic lies along the $F$-distribution curve. This requires that we compute the two $df$'s from the samples to define the shape of the $F$ distribution:\n$$\ndf_{Ref} = 8 - 1 = 7\n$$\n$$\ndf_{Cont} = 6 - 1 =5\n$$\n\nNow that we have the shape of the $F$-distribution defined, we can look up the probability of getting an $F$ statistic as extreme as ours (an F-distribution table can be used, or the value can be computed exactly using the function `pf()`, e.g. ` pf(2.12, 7, 5,lower.tail=FALSE)` where the values 7 and 5 are the $degrees\\; of\\; freedom$ for the reference sample and contaminated site sample respectively). \n\n```{r echo=FALSE, message=FALSE, fig.width=7, fig.height=3.0, warning=FALSE}\nOP <- par(\"mar\"=c(2,4,3,1), xpd=NA)\ndf1 = 7\ndf2 = 5\nFt  = 2.12  \nxmin = 0\nxmax = 10\np10  = qf(0.10,df1,df2)\np025  = qf(0.025,df1,df2)\np975  = qf(0.975,df1,df2)\nncurve.x = seq(xmin, xmax,length.out= 200)\nncurve.y = df(ncurve.x, df1, df2 )\nplot( ncurve.y ~ ncurve.x, type=\"l\", ylab=NA, xlab=NA,axes=FALSE,main=NA)\naxis(1, family= \"Architects Daughter\")\nlines(x = c(Ft,Ft), y=c(0,df(Ft,df1,df2)), col=\"red\"  , lty=1, lw=2)\nlines(x = c(p025,p025) , y=c(0,df( p025,df1,df2)) , col=\"grey\", lty=2, lw=2)\nlines(x = c(p975,p975) , y=c(0,df( p975,df1,df2)) , col=\"grey\", lty=2, lw=2)\n\n\ntext(x=Ft, y = df(Ft,df1,df2), eval(Ft), col=\"red\", pos=3, family= \"Architects Daughter\")\ntext(x=p025, y = df(p025,df1,df2), \"p=0.025\", col=\"#777777\", pos=1, family= \"Architects Daughter\")\ntext(x=p975, y = df(p975,df1,df2), \"p=0.975\", col=\"#777777\", pos=3, family= \"Architects Daughter\")\n\nncurve.x.mSE <- seq(Ft, xmax, length.out= 100)\nncurve.y.mSE <- df(ncurve.x.mSE, df1, df2 )\nncurve.x.mSE <- c(ncurve.x.mSE, max(ncurve.x.mSE), min(ncurve.x.mSE))\nncurve.y.mSE <- c(ncurve.y.mSE, 0,0)\npolygon( ncurve.x.mSE, ncurve.y.mSE, col=rgb(1, 0, 0,.3), border=NA)\npar(OP)\n```\n\nThe $F$ values associated with a probability of 0.025 and 0.975 (associated with rejection regions for a two-tailed $\\alpha$ of 0.05) are displayed on the curve in grey dashed vertical lines.\n\nThe probability of getting an $F$ as large as ours is about 0.21 (or 21%). Since $H_a$ represents _both_ sides of the distribution, we double the probability to give us the chance of getting a test statistic as great or as small as ours, so for a two-tailed test, $P=0.42$. With such a high $P$-value, we cannot reject the null and therefore can state that for all intents and purposes, the variances between both populations are the same (i.e. the observed variability between both $s$ can be explain by chance alone).\n\nThe following figure shows the observed $P$ values in both tails.\n\n```{r echo=FALSE, message=FALSE, fig.width=7, fig.height=3.0, warning=FALSE}\nOP <- par(\"mar\"=c(2,4,3,1), xpd=NA)\ndf1 = 7\ndf2 = 5\nFt  = 2.12  \nxmin = 0\nxmax = 10\nq.left = .522 # F value assoiatied with 21% probability (in the left tail)\nncurve.x = seq(xmin, xmax,length.out= 200)\nncurve.y = df(ncurve.x, df1, df2 )\nplot( ncurve.y ~ ncurve.x, type=\"l\", ylab=NA, xlab=NA,axes=FALSE,main=NA)\naxis(1, family= \"Architects Daughter\")\n#lines(x = c(Ft,Ft), y=c(0,df(Ft,df1,df2)), col=\"red\"  , lty=1, lw=2)\n\n#text(x=Ft, y = df(Ft,df1,df2), eval(Ft), col=\"red\", pos=3, family= \"Architects Daughter\")\ntext(x=3, y = df(2,df1,df2),  \"Upper 21%\\nregion\", col=rgb(1, 0, 0,.5), pos=4, family= \"Architects Daughter\")\ntext(x=0, y = df(.1,df1,df2), \"Lower\\n21%\\nregion\", col=rgb(1, 0, 0,.5), pos=2, family= \"Architects Daughter\")\n\n# Right side\nncurve.x.mSE <- seq(Ft, xmax, length.out= 100)\nncurve.y.mSE <- df(ncurve.x.mSE, df1, df2 )\nncurve.x.mSE <- c(ncurve.x.mSE, max(ncurve.x.mSE), min(ncurve.x.mSE))\nncurve.y.mSE <- c(ncurve.y.mSE, 0,0)\npolygon( ncurve.x.mSE, ncurve.y.mSE, col=rgb(1, 0, 0,.3), border=NA)\n\n# Left side\nncurve.x.mSE <- seq(0, q.left, length.out= 100)\nncurve.y.mSE <- df(ncurve.x.mSE, df1, df2 )\nncurve.x.mSE <- c(ncurve.x.mSE, max(ncurve.x.mSE), min(ncurve.x.mSE))\nncurve.y.mSE <- c(ncurve.y.mSE, 0,0)\npolygon( ncurve.x.mSE, ncurve.y.mSE, col=rgb(1, 0, 0,.3), border=NA)\npar(OP)\n```\n\nThis can be easily executed in `R` as a two-tailed test as shown in the following code block:\n\n```{r tidy=FALSE}\nRef <-  c(560, 530, 570, 490, 510, 550, 550, 530)\nCont <- c(600, 590, 590, 630, 610, 630)\nvar.test(Ref, Cont, alternative=\"two.sided\")\n```\n\nNote that the `var.test()` computes the $F$ ratio using the first variable name in the list as the numerator. For example, had we reversed the order of variables (i.e. `var.test(Cont, Ref, alternative=\"two-sided\")`), the returned $F$ value would be the inverse  of the original $F$ value, or $1/2.12 = 0.47$. The $P$ value would have stayed the same however.\n\n# Example 2\n\nAn investor is concerned that stock 1 is a riskier investment than stock 2 because its variation in daily prices is greater.  The following table is provided with summary statistics for a sample of 25 daily price changes. \n\n                     Stock 1   Stock 2\n------------------- --------- --------\nSample size          25        25\nStandard deviation   .76       .46\n\nIs stock 1's variability significantly greater than that of stock 2, or is the observed difference due to chance?  \n_[This example is adapted from McCLave et al. page 461]_\n\n## Solution to example 2\n\nWe are asked to test the hypothesis, $H_o$, that the two stock have equal variances and that any observed difference is due to chance (i.e. $\\sigma_1^2 = \\sigma_2^2$). The alternate hypothesis, $H_a$, states that stock 1 has greater variability than stock 2 (i.e. $\\sigma_1^2 > \\sigma_2^2$).\n\nSince we are given summary statistics of the samples and not the full dataset, we cannot use the `var.test()` function which requires the full dataset as input. Instead, we will compute the $F$ ratio and observed probabilities separately.\n\nThe $F$ ratio is:\n$$\nF = \\frac{(.76)^2}{(.46)^2} = 2.73\n$$\n\nThe degrees of freedom are $(25 - 1) = 24$ for both samples.\n\nThe probability of getting a test statistic as extreme as ours can be computed using the `pf()` function:\n\n```{r tidy=FALSE}\npf( 2.73, 24, 24, lower.tail = FALSE)\n```\n\nNote that we are using the  `lower.tail = FALSE` option since our alternate hypothesis is that $\\sigma_1^2 > \\sigma_2^2$. This gives us a probability of $0.008$, in other words, if the difference between stock 1 and stock 2 were explained by chance variability alone, there would be lest than a 1% chance of computing a $F$ ratio as extreme as ours. We can safely reject $H_o$ and state that the observed difference is real and that stock 1 has greater daily variability than stock 2.\n\n```{r echo=FALSE, message=FALSE, fig.width=7, fig.height=3.0, warning=FALSE}\nOP <- par(\"mar\"=c(2,4,3,1), xpd=NA)\ndf1 = 25\ndf2 = 25\nFt  = 2.73  \nxmin = 0\nxmax = 10\np10  = qf(0.10,df1,df2)\np025  = qf(0.025,df1,df2)\np975  = qf(0.975,df1,df2)\nncurve.x = seq(xmin, xmax,length.out= 200)\nncurve.y = df(ncurve.x, df1, df2 )\nplot( ncurve.y ~ ncurve.x, type=\"l\", ylab=NA, xlab=NA,axes=FALSE,main=NA)\naxis(1, family= \"Architects Daughter\")\nlines(x = c(Ft,Ft), y=c(0,df(Ft,df1,df2)), col=\"red\"  , lty=1, lw=2)\n\ntext(x=Ft, y = df(Ft,df1,df2), eval(Ft), col=\"red\", pos=3, family= \"Architects Daughter\")\n\nncurve.x.mSE <- seq(Ft, xmax, length.out= 100)\nncurve.y.mSE <- df(ncurve.x.mSE, df1, df2 )\nncurve.x.mSE <- c(ncurve.x.mSE, max(ncurve.x.mSE), min(ncurve.x.mSE))\nncurve.y.mSE <- c(ncurve.y.mSE, 0,0)\npolygon( ncurve.x.mSE, ncurve.y.mSE, col=rgb(1, 0, 0,.3), border=NA)\npar(OP)\n```\n\n\n# References\n\nFreedman D.A., Robert Pisani, Roger Purves. _Statistics_, 4th edition, 2007.  \nMcClave J.T., Dietrich F.H., _Statistics_, 4th edition, 1988.  \n\n-----\n\n**Session Info**:\n\n```{r,results='asis', echo=FALSE}\ndetach(\"package:extrafont\")\npander::pander(sessionInfo(), locale = FALSE)\n```\n\n\n[1]: http://www.amstat.org/sections/srms/pamphlet.pdf\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":false,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"number-sections":true,"output-file":"F_test.html"},"language":{},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.0.36","theme":["cosmo","custom.scss"],"minimal":true,"code-copy":"hover","title":"Comparing variances: Fisher's F-test"},"extensions":{"book":{"multiFile":true}}}}}